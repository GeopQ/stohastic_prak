\documentclass[11pt]{article}

\usepackage {a4wide}
\usepackage[utf8]{inputenc}
\usepackage[russian] {babel}
\usepackage{graphicx}
\usepackage{amsfonts} 
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{biblatex}
\usepackage{ tipa }
\usepackage{ upgreek }
\usepackage{indentfirst}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{ marvosym }

\definecolor{linkcolor}{HTML}{799B03} % цвет ссылок
\definecolor{urlcolor}{HTML}{799B03} % цвет гиперссылок

\hypersetup{pdfstartview=FitH,  linkcolor=linkcolor,urlcolor=urlcolor, colorlinks=true}

\newtheorem{theorem}{Теорема}
\newtheorem{definition}{Опредление} 
\newtheorem{statement}{Утверждение}
\newtheorem{proof}{Доказательство}
\newtheorem{properties}{Свойство}

\newcommand{\sign}{\normalfont{\text{sign}}}
\newcommand{\const}{\normalfont{\text{const}}}
\newcommand{\conv}{\normalfont{\text{conv}}}
\newcommand{\bern}{\normalfont{\text{Bern}}}
\newcommand{\bi}{\normalfont{\text{Bi}}}
\newcommand{\expon}{\normalfont{\text{Exp}}}
\newcommand{\pois}{\normalfont{\text{Pois}}}
\newcommand{\geom}{\normalfont{\text{Geom}}}
\newcommand{\cauchy}{\normalfont{\text{Cauchy}}}


\addbibresource{sample.bib}

\makeatletter
\newcommand{\myitem}[1]{%
\item[#1]\protected@edef\@currentlabel{#1}%
}
\makeatother


\numberwithin{equation}{section}
\begin{document}

\thispagestyle{empty}

\begin{center}
\ \vspace{-3cm}

\includegraphics[width=0.5\textwidth]{msu.eps}\\
{\scshape Московский государственный университет имени М.~В.~Ломоносова}\\
Факультет вычислительной математики и кибернетики\\
Кафедра системного анализа

\vfill

{\LARGE Отчёт по практикуму}

\vspace{1cm}

{\Huge\bfseries <<Стохастический анализ и моделирование.>>}
\end{center}

\vspace{1cm}

\begin{flushright}
  \large
  \textit{Студент 415 группы}\\
  М.\,М.~Савинов

  \vspace{5mm}

  \textit{Руководитель практикума}\\
  к.ф.-м.н., доцент С.\,Н.~Смирнов
\end{flushright}

\vfill

\begin{center}
Москва, 2022
\end{center}
\newpage




\tableofcontents
\newpage
\section{Задание 1}
\subsection{Формулировка задания}
\begin{enumerate}
  \item Реализовать генератор схемы Бернулли с заданной вероятностью успеха p. На осно- ве генератора схемы Бернулли построить датчик для биномиального распрделения.
  \item Реализовать генератор геометрического распределения. Проверить для данного рапределения свойство отсутствия памяти.
  \item Рассмотреть игру в орлянку - бесонечную послдеовательность независимых испы- таний с бросанием правильной монеты. Выигрыш $S_n$ определяетс как сумма по всем n испытаниями 1 и -1 в зависимости от выпавшей стороны Проиллюстрировать (в виде ломанной) поведение нормированной суммы $Y (i) = S_i/ \sqrt{(n)}$, как функцию от номера испытания $i = 1, \cdots,n$ для одной отдельно взятой траектории. Дать теоритическую оценку для $Y (n)$ при $n \to \infty$.
\end{enumerate}
\subsection{Генератор схемы бернулли }

\begin{definition}\label{4.23p154}
Схемой Бернулли называется эксперимент, в котором проводится, вообще говоря, неограниченное количество испытаний, при этом каждому испытанию присваивается бинарный признак ( 0 — успех, 1 — неудача), и выполняются следующие требования:
\begin{itemize}
  \item отсутствие взаимного влияния,
  \item воспроизводимость,
  \item испытания проводятся в сходных (но не идентичных) условиях.
\end{itemize}
\end{definition}
\begin{definition}
  Случайная величина X, принимающая значение 1 с вероятностью p и значение 0 с вероятностью $q = 1−p$, называется случайной величиной с распределением Бернулли (бернуллиевской случайной величиной).
\end{definition}


Для получения генератора схемы Бернулли с заданной вероятностью успеха p возьмем генератор равномерно распределенных на отрезке $[0, 1]$ случайных величин.
Сгенерируем равномерно распределенную случайную величину $X \sim U[0,1],$ тогда $Y \sim \bern (p) = \mathbb{I}(X < p), $ где $\mathbb{I}$ -- индикаторная функция:
\begin{equation}
  Y = \mathbb{I}(X<p) =
  \begin{cases}
    1,\ X<p,\\
    0,\ x \geq p.
  \end{cases}
\end{equation}
\begin{definition}
Случайная величина $X$ имеет биномиальное распределение с параметрами $n$ и $p$, $X \sim \bi(n,p),$ если 
\begin{equation}
  \mathbb{P}(X=k) = C^k_n p^k (1-p)^{n-k}, \ k \in \mathbb{N} \cup \{0 \}.
\end{equation} 
\end{definition}
Случайную величину X обычно интерпретируют как число успехов в серии из n оди- наковых независимых испытаний Бернулли с вероятностью успеха p в каждом из них.
Поэтому:

$$X = \sum_{i=1}^{n} Y_i,\\$$
где $X \sim \bi(n,p), \ Y \sim \bern(p).$
\subsection{Геометрическое распределение}
\begin{definition}
Случайная величина $X$ имеет геометрическое распределение с параметром $p, X \sim \geom(p),$ если
$$ \mathbb{P}(X=k) = (1-p)^k p = q^k p, \ k\in \mathbb{N} \cup \{0\}.$$
\end{definition}
Так же, как и в случае биномиального распределения, проводится некоторое количество испытаний Бернулли с одинаковой вероятностью успеха, до первого успеха. В качестве случайной величины с геометрическим распределением берем количество неудач до первого успеха.
\subsection{Свойство отсутствия памяти}
Приведем утверждение, интерпретирующее свойство отсутствия памяти.
\begin{statement}
  Пусть $Y \sim \geom,$ тогда $\forall m,n \in \mathbb{N} \cup {0}$ справедливо:
  $$\mathbb{P}(Y \leq m+n | Y \leq m) = \mathbb{P}(Y\leq n),$$
  то есть количество прошлых неудач не влияет на количество будущих неудач.
\end{statement}
\begin{proof}
  Распишем левую часть равенства:
  \begin{eqnarray}
    \mathbb{P}(Y \leq m+n | Y\leq m) = \frac{\mathbb{P}(Y\leq m+n, Y \leq m)}{\mathbb{P}(Y\leq m)} = \\
    = \frac{\mathbb{P}(Y \leq m+n)}{\mathbb{P}(Y \leq m)} = \frac{\sum_{i=m+n}^{\infty}q^i p}{\sum_{i=m}^{\infty}q^i p} = q^n\frac{\sum_{i=m}^{\infty}q^i p}{\sum_{i=m}^{\infty}q^i p} = q^n.
  \end{eqnarray}
  Распишем правую часть равенства:
  $$\mathbb{P}(Y\leq n) = \sum_{i=n}^{\infty}q^i p = p \frac{q^n}{1-q} = q^n. $$
\end{proof}
\subsection{Игра в орлянку}
Рассмотрим процесс игры в орлянку. Для этого смоделируем последовательность слу-
чайных величин $X_1, X_2, \dots, $ где 
\begin{equation}
  X_n =
  \begin{cases}
    1, \xi \in [0, 0.5),\\
    -1, \xi \in [0.5, 1]
  \end{cases}
\end{equation}
$\xi \sim U(0,1). $ Произведем $N=100000$ и построим траекторию процесса $Y(t), t \in [0,1],$
которая в точках $t_n, $где $n = 0, \dots, N равна$
$$ Y(t_n)= \frac{X_1+ \dots + X_n}{\sqrt{N}}$$
а в остальных случаях определяется с помощью кусочно-линейной интерполяции.
\subsection{Примеры работы программ}



%%% Задание 2
\section{Задание 2}
\subsection{Формулировка задания}
\begin{enumerate}
  \item Построить датчик сингулярного распределения, имеющий в качестве функции рас- пределения канторову лесницу. С помощью критерия Колмогорова убедиться в корректности работы датчика.
  \item Для канторовых случайных величин проверить свойство симметричности относительно $\frac{1}{2}$ ($X$ и $1−X$ распределены одинаково) и самоподобия относительно деления на 3 (условное распределение $Y$ при условии $Y \in [0, 1/3]$ совпадает с распределением $\frac{Y}{3}$ ) с помощью критерия Смирнова.
  \item Вычислить значение математическое ожидание и дисперсии для данного распре- деления. Сравнить теоритические значения с эмпирическими для разного объема выборок. Проиллюстрировать сходимость.
\end{enumerate}
\subsection{Построение датчика сингулярного распределения}
\begin{definition}
  Функция распределения называется сингулярной, если она непрерывна и ее множество точек роста имеет нулевую меру Лебега.
\end{definition}

Рассмотрим алгоритм построения канторова множества.
\begin{definition}
Из единичного отрезка $C_0=[0,1]$ удалим интервал $(\frac{1}{3},\frac{2}{3}).$ Множество $C_1 = [0, \frac{1}{3}] \cup [\frac{2}{3},1]$
состоит из двух отрезков; поступим также: теперь удалим из каждого отрезка его среднюю треть, и обозкачим получившеися множество через $C_2.$
На каждой итеррации будем повторять эту процедуру, получим последовательность замнутых множеств 
$C_0 \supset C_1 \supset C_2 \dots .$
Пересечение 
$$C = \cap_{i=0}^{+\infty} C_i$$
называется канторовым множеством.
\end{definition}

Канторово множество $C$ можно определить как множество чисел от нуля до единицы, представимое в троичной системе счисления с помощью нулей и двоек.
Посчитаем меру Лебега канторова множества. Вычтем длины отрезков, которые мы выкинули, из меры целого отрезка $[0,1]$:
$$ \mu(C) = 1 - \sum_{k=1}^{+\infty} \frac{2^{k-1}}{3^k}=1-1 = 0.$$
Таким образом, любая функция распределения, точки роста которой совпадает с точками канторова множества, является сингулярной. Как упоминалось ранее, точки канторова множества в троичной системе задаются только нулями и двойками, следовательно, все числа можно получить при помощи генератора случайной величины Бернулли следующим образом:
$$ x = \sum_{i=1}^{\infty}\frac{2\alpha_i}{3^i}, x \in C, \ \alpha_i \sim \bern(\frac{1}{2}).$$
Суммирование в формуле ведеться до бесконечности, т.к. ряд быстро сходиться, то будем обрубать подсчет при некотором $n \in \mathbb{N}.$

\subsection{Критерий Колмогорова}
Для проверки корректности работы датчика предлагается воспользоваться критерием Колмогорова. Будем считать максимальное по модулю отклонение эмпирической функции распределения от теоретической в точках выборки. 
Обозначим это число через $D_n.$ Для проверки простой гипотезы $H_0$ о соответсвии выборки известному закону распределения с заданным уровнем значимости 
$\alpha$ рассчитаем $p-\textsf{value}$ по статистике $\sqrt{n}D_n,$ то есть найдем значение функции распределения Колмогорова в заданной точке.
Для этого используем следующее представление для функции распределения Колмогорова:
$$ F_K(x)=1+2\sum_{k=1}^{+\infty}(-1)^k e^{-2k^2x^2}.$$
Гипотеза принимается, если $p= 1 -F_K(\sqrt{n}D_n)$ привышает заданный уровень значимости $\alpha$.
\begin{theorem}
  (Теорема Колмогорова)
  Пусть $X_1, \dots, X_n, \dots $ -- бесконечная выборка из распределения задаваемого непрерывной функциией распределения $F(x).$
  Пусть $F_n(x)$ -- выборочная функция распределения, построенная на первых $n$ элементах выборки. Тогда 
  $$ \sqrt{n} \sup_{x\in \mathbb{R}}|F_n(x)-F(x)| \to K$$
  по распрделению при $n \to + \infty,$ где $K$ -- случайная величина, имеющая распределение Колмогорова.
\end{theorem}
%% TODO
Доказательство можно найти в [1]. Результаты проверки корректоности датчика при уровне значимости
$\alpha = 0.05$ представленны в следующей таблице:
\\

\begin{tabular}{ | l | l | l | }
  \hline
  Кол-во запусков & Объем выборки & Принятие гипотезы\\ \hline
  $10^3$ & $10^3$ & 93.81\% \\
  \hline
  $10^3$ & $10^4$ & 95.32\%  \\
  \hline
  $10^4$ & $10^3$ & 94.72\% \\
  \hline
  $10^4$ & $10^4$ & 95.4\% \\
  \hline
\end{tabular}
\subsection{Свойства симметричности и самоподобия}
Покажем свойство симметричности. Пусть имеется канторова случайная величина
$X = \sum_{i=1}^{\infty} \frac{2\alpha_i}{3^i}, $ где $\alpha_i \sim \bern(\frac{1}{2}).$ Рассмотрим случайную величину $1-X$:
$$1-X = 1- \sum_{i=1}^{\infty}\frac{2\alpha_i}{3^i} = \sum_{i=1}^{\infty}-\sum_{i=1}^{\infty}\frac{2\alpha_i}{3^i}=\sum_{i=1}^{\infty}\frac{2(1-\alpha_i)}{3^i} = \sum_{i=1}^{\infty}\frac{2\beta_i}{3^i}.$$
Очевидно, что $\beta_i \sim \bern(\frac{1}{2}),$ поэтому случайные величины $1-X$ и $X$ имеют одинаковое распределение.

Покажем свойство самоподобия относительно деления на 3. Рассмотрим условное распределение случайной величины $Y$ на отрезке $[0,\frac{1}{3}].$
Это будет соотвесовать тому, что $\alpha_1 = 0.$ В таком случае:
$$ Y = \sum_{i=2}^{\infty}\frac{2\alpha_i}{3^{i}} = \sum_{i=1}^{\infty}\frac{2\alpha_{i+1}}{3^{i+1}} = \frac{1}{3} \sum_{i=1}^{\infty}\frac{2\alpha_i}{3^i} = \frac{1}{3}Y.$$
Что и означает самоподобие относительно деления на 3.

Проверим последние упомянутые свойства с помощью критерия Смирнова. Найдем
максимум поточечной разности двух эмперических функций распределения. Обозначив
полученное значение через $D_{nm},$ где $n,m$ -- длинны выборок, и посчитав $p-\textsf{value}$ по статистике 
$\sqrt{\frac{nm}{n+m}} D_{nm},$ для заданного уровня значимости $\alpha$ проверим простую гипотезу $H_0$ о том, что если обе выборки соответсвуют одинаковому закону распределния.
\begin{theorem}
  (теорема смирнова) Пусть $F_n^1(x), F_m^2(x)$-- эмпирические функции распредления с объемами выборок $n$ и $m$ 
  соответсвенно случайной величины $\xi.$  Тогда если $F(x) \in C^1(\mathbb{R}),$ тогд
  $$ \lim_{n,m \to + \infty} \mathbb{P}(\sqrt{ \frac{nm}{n+m} })D_{nm} \leq t = K(t) = \sum_{-\infty}^{+\infty}(-1)^j e^{-2j^2t^2}, \ \forall t >0$$
  где $D_{nm} = \sup_{x\in \mathbb{R}}|F_n^1-F_m^2|.$ 
\end{theorem}
Доказательство можно найти в [1].

Результаты проверки свойства симметричности при уровне значимости $\alpha = 0.05$ представленны в следующей таблице:
\\
\begin{tabular}{ | l | l | l | }
  \hline
  Кол-во запусков & Объем выборки & Принятие гипотезы\\ \hline
  $10^3$ & $10^3$ & 94.5\% \\
  \hline
  $10^3$ & $10^4$ & 94.1\%  \\
  \hline
  $10^4$ & $10^3$ & 96.02\% \\
  \hline
  $10^4$ & $10^4$ & 93.61\% \\
  \hline
\end{tabular}
\\
Результаты проверки свойства самоподобия при уровне значимости $\alpha = 0.05$ представленны в следующей таблице:

\\
\begin{tabular}{ | l | l | l | }
  \hline
  Кол-во запусков & Объем выборки & Принятие гипотезы\\ \hline
  $10^3$ & $10^3$ & 95.6\% \\
  \hline
  $10^3$ & $10^4$ & 95.7\%  \\
  \hline
  $10^4$ & $10^3$ & 96.06\% \\
  \hline
  $10^4$ & $10^4$ & 94.86\% \\
  \hline
\end{tabular}
\\
\subsection{Математическое ожидание и дисперсия}

Вычислим математическое ожидание и дисперсию рассматриваемой случайной величины. Как упоминалось ранее,
$F$ обладает свойством самоподобия, то есть при $0 < x < \frac{1}{3}$
выполнено соотношение $F(x) = F(3x)/2,$ а при $\frac{2}{3} < x < 1$ имеет равество 
$F(x) = \frac{1}{2} + \frac{F(3x-2)}{2}.$ Поэтому 
$$ \mathbb{E} \xi = \int_{-\infty}^{+\infty}x dF(x) = \int_{0}^{1/3}x dF(x)+ \int_{2/3}^{1}x dF(x) = \frac{1}{2} \int_{0}^{1/3}xdF(3x) + \frac{1}{2} \int_{2/3}^{1}xd(\frac{1}{2}+F(3x-2)).$$
Произведем замену $y=3x$ в первом интеграле, и $y=3x-2$ во втором интеграле:
$$ \mathbb{E} \xi = \frac{1}{2} \int_{0}^{1}\frac{y}{3} dF(y) + \frac{1}{2} \int_{0}^{1} \frac{y+2}{3} dF(y) = \frac{1}{6} \int_0^1ydF(y) +  \frac{1}{6} \int_0^1ydF(y) + \frac{1}{3} \int_0^{1}dF(y) = \frac{1}{3}\mathbb{E}\xi+\frac{1}{3}.$$
Таким образом, получем $\mathbb{E}\xi = \frac{1}{2}.$

Похожим образом вычислим дисперсию.
Для дисперсии верно тожество:
$$ \mathbb{D}\xi = \mathbb{E}\xi^2-(\mathbb{E}\xi)^2.$$
Вычислим $\mathbb{E}\xi^2$:
$$\mathbb{E}\xi^2 = \int_{0}^{1/3}x^2dF(x) + \int_{2/3}^1 x^2 dF(x) = \frac{1}{2} \int_0^1(\frac{y}{3})^2dF(y) + \frac{1}{2} \int_0^1(\frac{y+2}{3})^2dF(y)=$$
$$ \frac{1}{9} \mathbb{E}xi^2 + \frac{2}{9}\mathbb{E}\xi+\frac{2}{9}= \frac{1}{9} \mathbb{E}xi^2+\frac{1}{9}+\frac{2}{9}.$$
То есть имеем $\mathbb{E}\xi^2 = \frac{3}{8}.$ По формуле для дисперии получаем, 
$$ \mathbb{D}\xi = \frac{3}{8}-\frac{1}{2^2} = \frac{1}{8}.$$
\subsection{Примеры работы программ}

%% Задание 3
\section{Задание 3}
\subsection{Формулировка задания}
\begin{enumerate}
  \item Построить датчик экспоненциального распределения. Проверить для данного распределения свойство отсутствия памяти. Пусть $X_1, X_2, \dots, X_n$ — независимо экспоненциально распределенные с. в. с параметрами $\lambda_1, \lambda_2, \dots , \lambda_n$ соответственно. Найти распределение случайной величины $Y = min(X_1, X_2, \dots, X_n)$.
  \item На основе датчика экспоненциального распределения построить датчик пуассоновского распределения.
  \item Построить датчик пуассоновского распределения как предел биномиального распределения. С помощью критерия хи-квадрат Пирсона убедиться, что получен датчик распределения Пуассона.
  \item Построить датчик стандартного нормального распределения методом моделирования случайных величин парами с переходом в полярные координаты. Проверить при помощи t-критерия Стьюдента равенство математических ожиданий, а при помощи критерия Фишера равенство дисперсий.
\end{enumerate}
\subsection{Построение датчика экспоненциального распределения}
\begin{definition}
  Случайная величина $X$ имеет жкспоненциальное распределение с параметром $\lambda > 0,$ если ее функция распределения имеет вид:
  \begin{equation}
    F_X(x) = 
    \begin{cases}
      1-e^{-\lambda x}, \ x\geq 0,\\
      0, \ \ \ \ x<0
    \end{cases}
  \end{equation}
\end{definition}

\begin{theorem}
  Пусть на $\mathbb{R}$ определена функция $F(x)$ такая, что:
  \begin{enumerate}
    \item $F(x)$ непрерывна,
    \item $F(x)$ монотонно возрастает,
    \item $F(x) \to 0$ при $x \to -\infty,$
    \item $F(x) \to $ при $x \to \infty.$  
  \end{enumerate}
  Пусть так же задана случайная величина $Y \sim U[0,1].$ Тогда функция $F(x)$ является функция распределения случайной величины $X=F^{-1}(Y).$
\end{theorem}
Доказательтсво можно найти в [2].

Понятно, что функция распрделения $F_X(x)$ удовлетворяет условиям теоремы. Применим ее для моделирования датчина экспоненциального распределения на основе датчика равномерного распределения:
$$F_X(x)=1-e^{-\lambda x,}$$
$$ F_X^{-1}(y)=-\frac{1}{\lambda}\ln(1-y).$$
и если $Y \sim U[0,1],$ то получаем случайную величину, имеющую экспоненциальное распределение с параметром $\lambda$,
$$ X=-\frac{1}{\lambda}\ln(1-Y) \sim \expon(\lambda). $$
\subsection{Свойство отсутствия памяти}
\begin{definition}
  Случайная величина $X \sim \expon(\lambda)$ обладает свойством отсутсвия памяти, то есть
  $\forall t \not = 0$ и $\forall s$ следует, что 
  $$ \mathbb{P}(X \geq s+t | X \geq t) = \mathbb{P}(X \geq s).$$
\end{definition}
\begin{proof}
  По определению условной вероятности:
  $$\mathbb{P}(X \geq s+t|X \geq t) = \frac{ \mathbb{P} (X \geq s+t, X \geq t) }{ \mathbb{P} (X\geq t)} = \frac{\mathbb{P} (X \geq s+t)}{ \mathbb{P} (X \geq t)} = \mathbb{P}(X \geq s).$$

  Таким образом, получается 
  $$\mathbb{P}(X \geq s+t) = \mathbb{P}(X \geq t)\mathbb{P}(X \geq s).$$
  Для экспоненциально распределенной случайной величины верно, что:
  $$ \mathbb{P}(X\geq t) = 1 - F_X(t) = e^{\lambda t},$$
  $$ \mathbb{P}(X \geq s+t) = e^{-\lambda(s+t)}.$$
  Значит, выполняется 
  $$ e^{-\lambda(s+t)}=e^{-\lambda s} e^{-\lambda t}.$$
  Следовательно, экспоненциальное распределение обладает свойством отсутствия памяти.
\end{proof}
\subsection{Случайная величина $Y = \min(X_1,X_2, \dots,X_n)$}
\begin{statement}
  Пусть $X_1, X_2, \dots, X_n$ -- независимые экспоненциально распределённые
  случайные величины с параметрами $\lambda_1, \lambda_2, \dots, \lambda_n$ соответсвенно. Тогда случайная величина
  $Y = \min(X_1, X_2, \dots X_n) \sim \expon(\sum_{i=1}^n \lambda_i).$
\end{statement}
\begin{proof}
  $$F_Y(x) = \mathbb{P}(Y \leq x) = 1 - \mathbb{P}(Y>x) = 1 - \mathbb{P}(\min(X_1,X_2, \dots, X_n)>x)=$$
  $$ = 1 - \mathbb{P}(X_1>x, X_2>x, \dots,X_n>x) = \{X_1,X_2, \dots, X_n \text{независимы} \}=$$
  $$ 1- \mathbb{P}(X_1 > x) \cdot \dots \mathbb{P}(X_2>x) \cdot \dots \mathbb{P}(X_n>x)=$$
  $$ 1-(1-F_{X_1}(x))(1-F_{X_2}(x))\dots (1-F_{X_n}(x)) = 1 - e^{-\lambda_1 x} e^{-\lambda_2 x} \dots e^{-\lambda_n x} = 1 - e^{(\sum_{i=1}^{n}\lambda_i)x}.$$
\end{proof}

\subsection{Датчик пуассоновского распределения}
\begin{definition}
  Случайная величина $X$ имеет распределение Пуассона с парамтером $\lambda > 0,$ если
  $$\mathbb{P}(X=k) = \frac{\lambda^k}{k!}e^{-\lambda}, \ k \in \mathbb{N} \cup \{0\}.$$
\end{definition}
\begin{theorem}
  Пусть $X_1,X_2, \dots, X_n, \dots \sim \expon(\lambda)$ --независимые одинаково распредленные случайные величины. Тогда случайная величина, определенная следующим образом:
  $$Y = \max_{n \in \mathbb{N}} (S_n = X_1 + X_2+ \dots + X_n) < 1$$
  имеет распределение Пуассона с параметром $\lambda$ (при этом $Y = 0,$ если $ X_1\geq 1)$.
\end{theorem}

  Доказательство представлено в [2]. Таким образом, пуассоновскую случайную величину 
  $Y = n-1, Y \sim \pois(\lambda)$ можно получить, генерируя экспоненциально распределенные случайные величины $X_i \sim \expon(\lambda)$ до тех пор, пока их сумма не станет превышать 1.
\subsection{Датчик пуассоновского распределения как предел биномиального распределения. Проверка с помощью критерия хи-квадрат Пирсона}
\begin{definition}
  Сходимостью по вероятности последовательности случайных величин $\{\xi_n\}_{n=1}^{+\infty}$ к величине $\xi$ называется
  сходимость к нулю вероятности того, что $\xi_n$ не лежит в малой окрестности $\xi$, при произвольной малости этой окрестности:
  $$\forall \epsilon>0: \mathbb{P}(\{ |\xi_n-\xi|\}\geq \epsilon) \rightarrow_{n \to \infty} 0.$$
  Обозначение:  $\xi_n \overset{\mathbb{P}}{\to} \xi$
\end{definition}
Другой способ моделирования пуассоновской случайной величины основывается на предельном свойстве биномиального распределения:
$$ \bi(n,p) \overset{\mathbb{P}}{\to} \pois(\lambda), p = \frac{\lambda}{n} \to_{n \to \infty} 0.$$
Для того, чтобы проверить, что генерируется распределение Пуассона, будем исполь-
зовать критерий хи-квадрат Пирсона.
\begin{definition}
  Пусть случайные величины $\xi_1, \xi_2, \dots, \xi_n$ -- независимы, и каждая из них имеет стандартное 
  нормальное распределение $\mathcal{N}(0,1).$ Говорят, что случайная величина определенная как
  $$ \chi_n^2 = \xi_1^2+\xi_2^2+\dots+\xi_n^2,$$
  имееет распределение хи-квадрат с n степенями свободы.
\end{definition}

Критерий Пирсона заключается в следующем. Пусть $X$ -— дискретная неотрицательная случайная величина со следующим распределением:
$$ \mathbb{P}(X=k) =p_k, \ k \in \mathbb{N} \cup {0}.$$
Обозначим за $k_0$ максимальное значение в выборке из $n$ испытаний, за $n_k$ — количество элементов, принявших значение $k$. Тогда статистика критерия $\chi^2$ Пирсона выглядит следующим образом:
$$ X_n^2 = n \sum_{k=1}^{k_0} \frac{(\frac{n_k}{n} - p_k)^2}{p_k},$$
а сам критерий заключается в сравнении $X_n^2$ с общепринятыми критическими значениями для данной гипотезы.

Сгенерируем при помощи датчика пуассоновского распределения $n$
случайных величин, которые могут принимать значения $k$, $k \in \mathbb{N} \cup \{0\} $, и обозначим за $n_k$ то, сколько раз в выборке встречается значение $k$ (эмпирические частоты). Максимальное значение в выборке обозначим за $k_0$. Значит, каждый эксперимент (генерирование случайной величины построенным датчиком) имеет $k_0 + 1$ исходов. Выборочное среднее равно:
$$ \overline x = \frac{1}{n} \sum_{k=0}^{k_0}kn_k.$$ 
Примем в качестве оценки $\lambda = \overline{x}$ . Проверим основную гипотезу $H_0$ о том, что значения $k$ распределены по закону Пуассона (положим уровень доверия равным $\alpha$), то есть для случайной величины $X$, полученной при помощи датчика справедливо:
$$ p_k = \mathbb{P}(X=k) = \frac{\lambda^k}{k!}e^{-\lambda}, k= 0,1, \dots, k_0.$$
Теоритические частоты равны
$$ n_k^{'}=np_k.$$
Составим статистику хи-квадрат с $k_0$ степенями свободы:
$$ \chi_{k_0}^2=n\sum_{k=0}^{k_0} \frac{(\frac{n_k}{n}-p_k)^2} {p_k} = \sum_{k=0}^{k_0} \frac{(n_k-n_k^{'})^2}{n_k^{'}}.$$
Если значение $\chi_{k_0}^{2}$ будет меньше табличного значения критической точки $\chi(\alpha,k_0),$ то гипотеза 
$H_0$ о распределении Пуассона подтверждается.


Таблица соответствия средних эмпирических значений и критических точек для критерия Пирсона при уровне доверия $\alpha = 0.95$:
\\
\\
\begin{tabular}{ | l | l | l | }
  \hline
  Число степеней свободы & Эмпирическое значение & Критическая точка\\ \hline
  50 & 11.62 & 66.33 \\ \hline
  100 & 10.57 & 123.22  \\ \hline
  500 & 12.52 & 552.07 \\ 
  \hline
\end{tabular}
\\
\\
Таблица соответствия средних эмпирических значений и критических точек для кри- терия Пирсона при уровне доверия $\alpha$ = 0.95:
\\
\\
\begin{tabular}{ | l | l | l | }
  \hline
  Кол-во запусков & Объем выборки & Принятие гипотезы\\ \hline
  $10^3$ & $10^3$ & 95.8\% \\ \hline
  $10^3$ & $10^4$ & 95.5\%  \\ \hline
  $10^4$ & $10^3$ & 94.11\% \\ \hline
  $10^4$ & $10^4$ & 95.02\%\\
  \hline
\end{tabular}
\subsection{Датчик стандартного нормального распределения методом моде- лирования случайных величин парами с переходом в полярные координаты}
\begin{definition}
  Случайная величина $\xi$ имеет нормальное распределение вероятностей с параметрами $\mu$ и $\sigma^2,$ $\xi \sim \mathcal{N}(\mu,\sigma^2),$
  если ее плотность распределения задаеться формулой:
  $$ p_{\xi}(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp (-\frac{(x-\mu)^2}{2\sigma^2}), -\infty < x < +\infty.$$
\end{definition}

\begin{definition}
  Нормальное распределение с параметрами $\mu =0 $ и $\sigma^2 = 1$  стандартным нормальным распределением, и ее плотность распределения имеет следующий вид:
  $$ p_{\xi}(x) = \frac{1}{\sqrt{2\pi}} \exp (-\frac{(x)^2}{2}), -\infty < x < +\infty.$$
\end{definition}

Для моделирования стандартного нормального распределения рассмотрим случайную величину $X = {X_1, X_2} \sim \mathcal{N} (0, 1)$:
$$\mathbb{P}(X_1<x_1, X_2 < x_2) = \int_{-\infty}^{x_1}\int_{-\infty}^{x_2} \frac{1}{2\pi} e^{-\frac{\xi^2+\eta^2}{2}} d\xi d\eta.$$
Перейдем к полярным координатам:
$$\xi = \rho \cos \phi, \ \eta = \rho \sin \phi.$$
Вычислим якобиан заменты:
\begin{equation}
  J = 
  \begin{vmatrix}
    \cos \psi & -\rho \sin \psi\\
    \sin \psi & \rho \cos \psi 
  \end{vmatrix}
  = \rho \not = 0.
\end{equation}
Тогда 
\begin{eqnarray}
  \mathbb{P}(X_1<x_1, X_2 < x_2) = \int_{\rho \cos \psi < x_1, \rho \sin \psi < x_2} \frac{1}{2\pi} e^{-\frac{\xi^2+\eta^2}{2}} d\xi d\eta = \\
  \{\omega= \rho^2\} = \int_{\sqrt{\omega} \cos \psi < x_1, \sqrt{\omega} \sin \psi < x_2} \frac{1}{4\pi} e^{-\frac{\omega}{2}} d\omega d\phi.
\end{eqnarray}
Подынтегральное выражение является произведением плотностей случайных величин
$Y_1 \sim \expon(\frac{1}{2})$ и $Y_2 \sim U[0,2\pi].$ Таким образом, совместное распределение случайных величин $X_1$ и $X_2$ совпадает с совместным распределением 
$$ \{ \sqrt{Y_1}\cos Y_2, \sqrt{Y_1}\sin Y_2\}, \ Y_1 \sim \expon (\frac{1}{2}), \ Y_2 \sim U[0,2\pi].$$ 
Слуйчаные величины $X_1$ и $X_2$ являются независимыми, поскольку их совместное распределение равно произведению их маргинальных распределений:
$$\mathbb{P}(X_1<x_1, X_2 < x_2) = $$
$$\int_{-\infty}^{x_1}\int_{-\infty}^{x_2} \frac{1}{2\pi} e^{-\frac{\xi^2+\eta^2}{2}} d\xi d\eta =  \frac{1}{2\pi} \int_{-\infty}^{x_1} e^{-\frac{\xi^2}{2}} d\xi \int_{-\infty}^{x_2} e^{-\frac{\eta^2}{2}} d\eta.$$
\subsection{t-критерий Стьюдента и критерий Фишера}
Проверим равенство математических ожиданий построенных случайных величин $\xi$ и $\eta$, используя t-критерий Стьюдента. 
Обозначим через $M_1$ и $M_2$ математические ожидания первой и второй выборки соответственно. 
Рассмотрим разность выборочных средних $\Delta = \overline{\xi} − \overline{\eta}$.
Eсли нулевая гипотеза о равенстве математических ожиданий выполнена, то математическое ожидание $\mathbb(\Delta) = M_1 − M_2 = 0$. Зная, что $\mathbb{D}(\xi) = \mathbb{D}(\eta) = 1$, то

$$\mathbb{D}(\overline{\xi}) = \frac{\mathbb{D}(\sum_{i=1}^n x_i)}{n^2} = \{ X_1, \dots, X_n \text{независимы}\} = \frac{1}{n} = \mathbb{D}(\overline{\eta}),$$
то тогда  
$$ \mathbb{D}(\Delta) = \frac{2}{n}.$$
Используя несмещённую оценку дисперсии: $s_1^2= \frac{\sum_{i=1}^{n}(\xi_i-\overline{\xi})^2}{n-1}$ и 
$s_2^2= \frac{\sum_{i=1}^{n}(\eta_i-\overline{\eta})^2}{n-1},$
получаем несмещенную оценку дисперсии разности выборочных средних: $s_{\Delta}^2 = \frac{s_1^2+s_2^2}{n}.$
Значит для проверки нулевой гипотезы t-статистика равна $t=\frac{\overline{\xi}-\overline{\eta}}{\sqrt{s_1^2+s_2^2}} \sqrt{n}.$
Если полученное значение статистики $t$ превосходит критическое значение $t_{\alpha,r}$ для заданного уровня значимости $\alpha$, то нулевая гипотеза отвергается.


Результаты проверки равенства математических ожиданий с помощью критерия Стьюдента с $\alpha = 0.05$ представленны в следующей таблице:
\\
\begin{tabular}{ | l | l | l | }
  \hline
  Кол-во запусков & Объем выборки & Принятие гипотезы\\ \hline
  $10^3$ & $10^3$ & 93.7\% \\ \hline
  $10^3$ & $10^4$ & 95.9\%  \\ \hline
  $10^4$ & $10^3$ & 95.31\% \\ \hline
  $10^4$ & $10^4$ & 94.0\% \\ \hline
  \hline
\end{tabular}
\\


Теперь опишем критерий Фишера равенства дисперсий. Для ранее определённых величин $s_1^2$ и $s_2^2$ зададим статистику
$$ F =\frac{s_1^2}{s_2^2}.$$
Проверим равенство дисперсий с помощью критерия Фишера.
Если $F < F_{\alpha/2}(n−1,n−1)$ или $F > F_{1−\alpha/2}(n − 1, n − 1)$, то нулевая гипотеза о равенстве дисперсий отвергается, где $F_{\alpha}(n − 1, m − 1)$ есть $\alpha$-квантиль распределения Фишера.
Результаты проверки равенства дисперсий с помощью критерия Фишера с $\alpha$ = 0.95 представленны в следующей таблице:
\\
\begin{tabular}{ | l | l | l | }
  \hline
  Кол-во запусков & Объем выборки & Принятие гипотезы\\ \hline
  $10^3$ & $10^3$ & 94.7\% \\ \hline
  $10^3$ & $10^4$ & 95.8\%  \\ \hline
  $10^4$ & $10^3$ & 95.62\% \\ \hline
  $10^4$ & $10^4$ & 96.91\% \\ \hline
  \hline
\end{tabular}
\\
\subsection{ Примеры работы программы}





\section{Задание 4}
\subsection{Формулировка задания}
\begin{enumerate}
  \item Построить датчик распределения Коши.
  \item На основе датчика распределения Коши с помощью метода фон Неймана построить датчик стандартного нормального распределения. При помощи функции normal probabitity plot убедиться в корректности построенного датчика и обосновать наблюдаемую линейную зависимость.
  \item Сравнить скорость моделирования стандартного нормального распределения в заданях 3 и 4.
\end{enumerate}
\subsection{Датчик распределения Коши}
\begin{definition}
  Случайная величина $X$ имеет распределение Коши с параметрами $a$ и $b$, если ее функция распределения имеет вид:
  $$F_{X}(x) = \frac{1}{\pi}\arctan (\frac{x-a}{b})+\frac{1}{2}.$$
\end{definition}
Плотность распределения Коши имеет следующий вид:
$$ p_X(x) = \frac{1}{\pi} \frac{b}{(x-a)^2+b^2}.$$
Функция распределения $F_X(x)$ удовлетворяет условиям теоремы 3. Обратная функция для $F_X(x)$ равна $F_X^{-1}(y) = a+b\tan(\pi(y-\frac{1}{2})).$Следовательно, в качетсве датчика распределения Коши можно построить датчик случаной величины $X = F_X^{-1}(Y),$ 
где $Y \sim U[0,1].$
\subsection{Метод фон Неймана}
Метод фон Неймана заключается в моделировании нормального распределения путём мажорирования плотностью распределения Коши с параметрами a и b. Для достижения наилучшей оценки, начнем подбирать параметры a и b.
Плотность стандартного нормального распределения $p_1(x)$ и плотность распределения Коши $p_2(x)$ выглядит следующим образом:
$$ p_1(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}},$$
$$ p_2(x) = \frac{1}{\pi} \frac{b}{(x_a)^2+b^2}.$$
При моделировании будем следовать такому алгоритму:
\begin{enumerate}
  \item возьмем некоторое число $k > 0,$ такое что $p_1(x)\leq kp_2(x), \forall x \in \mathbb{R},$
  \item рассмотрим значение случайной величины $x=X, X \sim \cauchy (a,b), $
  \item Сгенерируем случайную величину $y = Y(x) \sim \bern(\frac{p_1(x)}{kp_2(x)}),$
  \item Если $y=1$, то $x$ -- значение из распределения с плотностью $p_1(x),$ иначе продолжаем моделирование, начиная с пункта 2. 
\end{enumerate}
Данный алгоритм работает тем быстрее, чем ближе отношение $\frac{p_1(x)}{kp_2(x)}$ к единице, поэтому в качетсве $k$
возьмем $k^* = \min_{a,b} \max_{x} \frac{p_1(x)}{p_2(x)}.$Рассмотрим отношение 
$$\frac{p_1(x)}{p_2(x)} = \frac{\sqrt{\pi}}{\sqrt{2}b} e^{\frac{-x^2}{2}}((x-a)^2+b^2).$$ 
Пусть $a=0.$ Рассмотрм вспомогательную функцию:
$$g(x) = e^{\frac{-x^2}{2}}(x^2+b^2).$$
Найдем максимум этой функции:
$$ g'(x) =  e^{\frac{-x^2}{2}} x (2-b^2-x^2)=0,$$
значит точки экстремума:
$\left[ 
      \begin{gathered} 
        x=0, |b| > \sqrt{2} \\ 
        x = \pm \sqrt{2-b^2}, 0<|b|\leq \sqrt{2}. \\ 
      \end{gathered} 
\right.$
Таким образом 
$$ k^* = min \{ min_{|b|>\sqrt{2}} \sqrt{\frac{\pi}{2}}b, min_{0\leq |b| \leq \sqrt{2}} \frac{\sqrt{2\pi}}{b}e^{\frac{b^2}{2}-1} \}.$$
Поскольку $k>0,$ то и $b>0.$ Найдем максимум вспомогательной функции 
$$h(b) = \frac{e^{\frac{b^2}{2}-1}}{b}:$$
$$\frac{1-b^2}{b^2}e^{\frac{b^2}{2}-1},$$
следовательно, поскольку $b>0,$ точкой экстремума являеться $b=1.$
Получается оптимум при $a^*=0,b^*=1:$
$$ k^* = min \{ \sqrt{pi}, \sqrt{\frac{2\pi}{e}}.  \}$$
Докажем, что $a=0$ -- оптимальное значение параметра.
\begin{eqnarray}
  k^* = \min_{a,b} \max_{x} ( \frac{\sqrt{\pi}}{\sqrt{2}b}e^{\frac{-x^2}{2}} ((x-a)^2)+b^2) )=\\
  =\min_{a} \{  \min_{b>\sqrt{2}} \frac{p_1(x)}{p_2(x)} \bigg|_{x=0}, min_{0<b\leq \sqrt{2}} \frac{p_1(x)}{p_2(x)} \bigg|_{x=\pm \sqrt{2-b^2}}   \} > \\
  > min_{a} \{ min_{b>\sqrt{2}} \frac{\sqrt{\pi}}{\sqrt{2}b} (a^2+b^2), min_{0<b\leq \sqrt{2}} (\sqrt{2-b^2} + |a|)        \}
\end{eqnarray}

Минимум выражения достигается при $a = 0$.
Иллюстрацияработыпостроенногодатчика, использующая функцию \textsf{normal probability plot},
представлена в пункте "Примеры работы программы".
График функции распределения стандартной нормальной случайной величины представляет прямую.
На оси абсцисс откладываются точки выборки, на оси ординат —- квантили стандартного нормального распределения.

Возьмем случайную величину $\xi \sim N(\mu, \sigma^2).$ Тогда функция распределения 
$$ F_{\xi}(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \int_{-\infty}^{x} e^{-\frac{(t-\mu)^2}{2\sigma^2}}dt.$$
Введем замену переменной $s= \frac{t-\mu}{\sigma},$ тогда
$$ F_{\xi}(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\frac{x-\mu}{\sigma}} e^{-\frac{s^2}{2}}ds = F(\frac{x-\mu}{\sigma}).$$
где $F(x)$ -- функция стандартного нормального распределения.


Таким образом, квантили различных распределений связаны между собой линейно, что означает, что любую нормальную случайную величину
$\xi \sim N(\mu, \sigma^2)$ можно представить в виде $\xi = \sigma \eta + \mu,$ где $\eta \sim N(0,1),$ а Функция
\textsf{normal probability plot}  будет прямой со сдвигом $\mu,$ и с коэффициентом наклона $\sigma.$
\subsection{Сравнение скоростей работы}

\subsection{Примеры выполенения программ}


\section{Задание 5}
\begin{enumerate}
  \item Пусть $X_i \sim N(\mu, \sigma^2).$ Убедиться эмпирически в справедливости ЗБЧ и ЦПТ, т.е.
  исследовать поведеление суммы $S_n$ и эмпирического распределения величины
  $$ \sqrt{n} (\frac{S_n}{n}-a).$$
  \item Считая $\mu$ и $\sigma^2$ неизвестнымим, для пункта 1 построить доверительные интервалы для среднего и диспрерисии.
  \item Пусть $X_i \sim K(a,b)$ имеет распределение Коши со сдвигом $a$ и маштабом $b$. Проверить эмпирически, как ведут себя суммы $S_n/n.$
  Результат объяснить, а также найти закон распределения данных сумм.
\end{enumerate}
\subsection{ЗБЧ и ЦПТ для нормального распределения}
Пусть $X_i \sim \mathcal{N}(\mu,\sigma^2).$ Исследуем поведение суммы $\frac{S_n}{n}$ и эмпирического распределения
величины 
$$ \sqrt{n}(\frac{S_n}{n}-\mu).$$
\begin{theorem}(Закон больших чисел)
  Пусть $X_1, X_2, \dots $-- независимые одинаково распределенные случайные величины, $\mathcal{E}X_i=\mu,$\ 
  $\forall i \in \mathbb{N}, |\mu| < \infty, S_n=X_1+\dots+X_n.$ Тогда $\frac{S_n}{n} \to_{n \to \infty}^{\mathbb{P}} \mu,$ т.е.
  $$ \forall \epsilon>0 \mathbb{P}(|\frac{S_n}{n}-\mu|\geq) \to_{n \to \infty} 0.$$
\end{theorem}


\begin{theorem}(Центральная предельная теорема)
  Пусть $X_1,X_2, \dots$-- независимые одинаково распределенные случайные величины, $0<\mathbb{E}X_i^2< \infty, \forall i \in \mathbb{N}, S_n=X_1+\dots X_n.$ Тогда
  $$ \mathbb{P} (\frac{S_n-\mathbb{E}S_n}{\sqrt{\mathbb{D}S_n}}) \to_{n \to \infty} \ \Phi(x), \ x \in \mathbb{R},$$
  где $\Phi(x)--$ функция стандартного нормального распределения:
  $$ \Psi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{u^2}{2}}du.$$
\end{theorem}
Доказательство этих теорем можно найти в [2].
\subsection{Доверительные интервалы для среднего и дисперсии}
Рассмотрим случайную величину $X \sim \mathcal{\mu, \sigma^2}.$ Пусть $x_1, x_2, \dots, x_n$ --
ее реализации.
Введем обозначения:
\begin{itemize}
  \item $$ \overline{X} = \frac{1}{n} \sum_{i=1}^{n} x_i;$$
  \item Выборочная дисперсия $$ s^2 = \frac{1}{n-1} \sum_{i=1}^{n}(x_i-\overline{x})^2.$$
\end{itemize}
Для параметров строятся следующие доверительные интервалы:
\begin{enumerate}
  \item Для неизвестного среднего $\mu$ при неизвестной дисперсии $\sigma^2:$
  $$ \overline{x}- \frac{s}{\sqrt{n}}t_{\gamma} < \mu < \overline{x} + \frac{s}{\sqrt{n}}t_{\gamma},$$
  где $t_{\gamma}$-- критическая точка распределения Стьюдента (распределения Коши с параметрами $a=0, b=1$) с $n-1$ 
  степенями свободы  и уровнем значимости $\alpha = 1 -\gamma;$
  \item Для неизвестной диспрерисии:
  $$ \frac{(n-1)s^2}{\chi_{\frac{\alpha}{2}, n-1}^{2}} < \sigma^2 < \frac{(n-1)s^2}{\chi_{\frac{1-\alpha}{2}, n-1}^{2}},$$  
  где $\chi_{n-1}^2$ -- критические точки $\chi^2-$распрделения с $n-1$ степенями свободы и соответветсвующими уровнями значимости $\alpha = 1 -\gamma.$
\end{enumerate}

\subsection{ЗБЧ для распределения Коши}
Пусть $X_i \sim \cauchy(a,b), \forall i \in \mathbb{N}.$ Рассмотрим график, на нем видно, что $ \frac{S_n}{n}$ не имеет
предела, то есть закон больших чисел для распределения Коши не выполняется.
Заметим, что это можно объяснить тем, что мы не можем найти математического ожидания от нашей случайной величины, то есть, мы нарушает одно из условий теоремы о ЗБЧ. 
Докажем, что математическое ожидание случайной величины $X \sim \cauchy(a,b)$ не являеться конечным:
$$ \mathbb{E}X = \frac{1}{\pi} \int_{-\infty}^{\infty} \frac{bx}{(x-a)^2+b^2}dx = \frac{b}{2\pi} \ln((x-a)^2+b^2) \bigg|_{-\infty}^{\infty} = \infty - \infty.$$
Следовательно, эмпирический результат соответсвуют теории.
\begin{theorem}
  Если $\xi_1 \sim \\cauchy(a_1,b_1), \xi_2 \sim \cauchy(a_2,b_2),$ то $\xi_1+\xi_2 \sim \cauchy(a_1+a_2, b_1+b_2).$ Или, что то же самое,
  $$ \cauchy(a_1,b_1)* \cauchy(a_2,b_2) = \cauchy(a_1+a_2, b_1+b_2),$$
  где * -- операция свёртки.
\end{theorem}
Доказательство можно найти в [5].

Из этой теоремы следует следующие свойство распределения Коши:
\begin{properties}
  Если случайные величины $\xi_1, \dots, \xi_n$ независимы и имеют все одно и то же распределение Коши, то среднее арифметическое 
  $\overline{xi} = \frac{1}{n} \sum_{i=1}^{n}\xi_i$ имеет то же распределение, что и каждое $\xi_j.$
\end{properties}
\subsection{Примеры работы программы}
ф
\section{Задание 6}
\subsection{Формульровка задания}
\begin{enumerate}
   \item Посчитать интеграл 
  $$ \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} \cdots \int_{-\infty}^{+\infty} \frac{e^{-(x_1^2+\cdots+x_{10}^2)+\frac{1}{2^7 \cdot x_1^2 \dot x_1^2 \cdots \cdot x_{10}^2}   }}{x_1^2 \cdot \dots \cdot x_{10}^2} dx_1 dx_2 \dots dx_{10}$$
  \begin{itemize}
    \item методом Монте-Карло,
    \item  методом квадратур, сводя задачу к вычислению собственного интеграла Римана
  \end{itemize}
  \item Для каждого случая оценить точность вычислений.
\end{enumerate}
\subsection{Метод Монте-Карло}
Перепишем интеграл
$$ \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} \cdots \int_{-\infty}^{+\infty} \frac{e^{-(x_1^2+\cdots+x_{10}^2)+\frac{1}{2^7 \cdot x_1^2 \dot x_1^2 \cdots \cdot x_{10}^2}   }}{x_1^2 \cdot \dots \cdot x_{10}^2} dx_1 dx_2 \dots dx_{10}$$
в виде 
$$ \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} \cdots \int_{-\infty}^{+\infty} f(x_1, \dots, x_{10})g(x_1, \dots, x_{10}) dx_1 dx_2 \dots dx_{10},$$
где 
$$ f(x) = \sqrt{\pi^{10}} \cdot \frac{e^{-frac{1}{2^7 \cdot x_1^2 \cdot \dots \cdot x_10^2} }} {x_1^2 \cdot \dots \cdot x_10^2}, \ g(x) = \frac{1}{\sqrt{\pi^10}} \cdot e^{-(x_1^2+\dots+x_{10}^2)}.$$
Заметим, что $g(x)$ является совместной плотностью распределения набора независимых случайных величин, имеющих нормальное распределение с параметрами 0 и $\frac{1}{2}:$
$$ x = (x_1, \dots, x_{10}, \ x_i \sim \mathcal{N}(0,\frac{1}{2})).$$
Тогда интеграл можно переприсать в виде 
$I = \mathbb{E} f(x_1, \dots, x_{10}), \ x_i \sim \mathcal(0, \frac{1}{2}).$


Рассмотрим выборку 
$$ x^{i} = (x_1^i, \dots, x_{10}^{i}), \ x_k^{i} \sim \mathcal{N}(0, \frac{1}{2}),\ k = \overline{1,10}, \ i = \overline{1,n}.$$
Согластно ЗБЧ выборочное среднее будет стремиться к математическому ожиданию, то есть:
$$ \overline{f} = \frac{S_n}{n} = \frac{1}{n} \sum_{i=1}^{n}f(x^{i}) \to_{n\to \infty} I. $$
Оценим погрешность метода Монте-Карло с помощью центральной предельной теоремы:
\begin{eqnarray}
  \mathbb{P}(|\frac{S_n}{n}-I| < \epsilon) = \mathbb{P}(|\frac{S_n-nI}{n}| < \epsilon) = \mathbb{P}(|\frac{S_n-nI}{\sigma\sqrt{n}}|< \frac{\sqrt{n}}{\sigma}\epsilon)=\\
  = \mathbb{P}(-\frac{\sqrt{n}}{\sigma}\epsilon< \frac{S_n-nI}{\sigma\sqrt{n}}< \frac{\sqrt{n}\epsilon})=\Psi_0(\frac{\sqrt{n}}{\sigma}\epsilon) - \Psi_0(-\frac{\sqrt{n}}{\sigma}\epsilon)=\\
  = \Psi_0(\frac{\sqrt{n}{\sigma}\epsilon})- (1- \Psi_0(\frac{\sqrt{n}{\sigma}\epsilon})) = 1 - 2\Psi_0(\frac{\sqrt{n}{\sigma}\epsilon}) = 1-\Psi_0(x_p) = \alpha,
\end{eqnarray}
где 
\begin{itemize}
  \item $\Psi_0(x)$ - функция Лапласа или функция ошибок:
  $$ \Psi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{t^2}{2}}dt,$$
  \item $x_p = \frac{\sqrt{n}}{\sigma}\epsilon$ -- квантиль уровня $p,$ есть решение уравнения
  $$ \Psi_0(x_p)=p,$$
  \item $\alpha$ --уровень доверия.
\end{itemize}
Погрешность $\epsilon$ для соответсутющего уровня доверия $\alpha = 1- 2\Psi_0(x_p)$ связана с $x_p$ соотношением:
$$ \epsilon = \frac{\sigma x_p}{\sqrt{n}}.$$
Значение $\sigma > 0$ используем как значение выборочной дисперсии:
$$ \sigma = \frac{1}{n}\sum_{i=1}^{n}f^2(x_i)- (\frac{1}{n} \sum_{i=1}^n f(x_i))^2.$$
В качестве значения уровня доверия возьмем $\alpha = 0.99.$ 
$$ \mathbb{P}(|\frac{S_n}{n}-I|< \epsilon)= \alpha = 0.99.$$
Ниже приведена таблица зависимости вычисленных значений интеграла и полученной погрешности при разном количестве испытаний:
\\
\begin{tabular}{ | l | l | l | l |}
  \hline
  Число испытаний & Результат & Погрешность & Время работы \\ \hline
  $10^2$ & 356.26 & 79.4061 & 0.0074 \\ \hline
  $10^3$ & 157.706 & 31.0156 & 0.0098 \\ \hline
  $10^4$ & 139.156 & 19.266 & 0.1216 \\ \hline
  $10^5$ & 119.4103 & 6.9206 & 0.4675 \\ \hline
  $10^6$ & 124.4291 & 2.3284 & 3.0683 \\ \hline
  $10^7$ & 124.5648 & 0.7101 & 16.911 \\
  \hline 
\end{tabular}
\\
\subsection{Метод квадратур}
Сведем задачу к вычислению собственного интеграла Римана. Для этого сделаем следующую замену переменных:
$$ x_i =tg(\frac{\pi}{2}t_i), t_i \in [0,1]. $$
Таким образом, по методу прямоугольников исходный интеграл приблизится значением:

$$ I = (\frac{\pi}{2})^{10} \int_{-1}^{1} \cdots \int_{-1}^{1} \frac{exp\{- (\sum_{k=1}^{10} tg(\frac{\pi}{2}t_k)^2 + \frac{1}{2^7 \prod_{k=1}^{10} tg (\frac{\pi}{2}t_k)^2}    )   \}}{   \prod_{k=1}^{10} tg (\frac{\pi}{2}t_k)^2} \cdot \prod_{k=1}^{10}\cos(\frac{\pi}{2}t_k)^2  }   dt_1 \dots dt_{10}$$
Проведем равномерное разбиение отрезка $[-1,1]$ на $N$ частей:
$$ -1 = t_0 < t_1 < \dots < t_N =1, \ t_i = -1 + i \frac{1-(-1)}{N} = i\frac{2}{N}-1.$$
Обозначим через $f(t_1, \dots t_10)$ подынтегральную функцию интеграла $I$.
Будем использовать метод средних прямоугольников. Для этого нам необходимо выбрать середины нашего разбиения:
$$ y_i = \frac{t_i+t_{i-1}}{2}, \ i = \overline{1,N}.$$
Тогда наш интеграл приближённо можно посчитать следующим образом:
$$ I_N = (\frac{\pi}{N})^10 \sum_{i_1=1}^{N} \cdots \sum_{i_10=1}^N f(y_{i_1}, \dots, y_{i_10}).$$
Оценка погрешности метода прямоугольников на равномерной сетке имеет следующий вид:
$$ \epsilon = \frac{h^2}{24}(b-a) \sum_{i,j=1}^{10}\max{|f''_(x_i,x_j)|} = \frac{1}{N^2} \sum_{i,j=1}^{10} max|f''_{x_i,x_j}|.$$


Приведем таблицу зависимости результата от количества точек разбиения отрезка:
\\
\begin{tabular}{ | l | l | l | }
  \hline
  N & Результат & Время работы \\ \hline
  3 & 0.008 & 0.5518 \\ \hline
  4 & 5012.2123 & 4.9402\\ \hline
  5 & 183.4886 & 45.398591 \\ \hline
  \hline
\end{tabular}
\\
Вывод: метод Монте–Карло работает намного эффективнее по скорости, чем метод квадратур.
\section{Задание 7}
\subsection{Формулировка задания}
\begin{enumerate}
  \item Методом случайного найти минимальное значение функции $f$ на множестве 
  $A = \{ x_1, x_2: x_1^2+x_2^2\leq 1\},$ т.е. $y=\min f(x),$где 
  $$ f(x) = x_1^3 \sin(\frac{1}{x_1})+10x_1x_2^4cos(\frac{1}{x_2}),$$
  при $x_1 \not = 0$ и $x_2 \not = 0,$ функция допределяеться по неприрывности при $x_1=0$ или $x_2=0.$
  \item Методом имитации отжига найти минимиальное значение функции Розенброка $g$ в пространстве $R^2,$ где 
  $$ g(x) = (x_1-1)^2+100(x_2-x_1^2)^2$$
  \item Оценить точность. Сравнить результаты со стандартными метродами оптимизации.
\end{enumerate}
\subsection{Метод случайного поиска}
Возьмем единичный круг и сгенерируем на нём набор равномерно распределенных по нему точек.
Найдем миниммальное значение. Совместная плотность равномерного распределения случайных величин $x_1, x_2$ на единичном круге равнаЖ
\begin{equation*}
  f_{x_1,x_2} = 
   \begin{cases}
     \frac{1}{\pi}, & x_1^2+x_2^2 \leq 1,\\
     0, & \text{иначе.}
   \end{cases}
\end{equation*}
В полярных координатах:
\begin{equation*}
   \begin{cases}
     x_1 = r \cos \phi, & r \leq 1,\\
     x_2 = r \sin \phi, & 0 \leq \phi < 2\pi.
   \end{cases}
\end{equation*}
Таким образом, получим:
$$ \mathbb{P} ((x_1,x_2)\in A) = \iint_{x_1^2+x_2^2\leq1} \frac{1}{\pi} dx_1 dx_2 = \frac{1}{\pi} \int_0^1 r dr\int_0^{2\pi} d\phi = \int_0^1 dr^2 \int_0^{2\pi} \frac{1}{2\pi} d\phi.$$
Сделаем замену 
$$ q = r^2, r = \sqrt{q}, \ q \in [0,1].$$
Тогда выражение принимет вид:
$$\mathbb{P} ((x_1,x_2)\in A) = \int_0^1 dq \int_0^{2\pi} \frac{1}{2\pi} d\psi.$$
Следовательно, $x_1$ и $x_2$ выражаются в виде:
\begin{equation*}
  \begin{cases}
    x_1 = \sqrt{q} \cos \phi, & q \sim U[0,1],\\
    x_2 = \sqrt{q} \sin \phi, & \phi \sim [0,2\pi].
  \end{cases}
\end{equation*}
\subsection{Метод имитации отжига}
Алгоритм основывается на имитации физического процесса, который происходит при кристаллизации вещества, в том числе при отжиге металлов.
Предполагается, что атомы уже выстроились в кристалличекую решётку, но ещё допустимы переходы отдельных атомов из одной ячейки в другую.
Предполагается, что процесс протекает при постепенно понижающейся температуре.
Переход атома из одной ячейки в другую происходит с некоторой вероятностью, причём вероятность понижается с понижением температуры.
Устойчивая кристаллическая решётка соответствует минимуму энергии атомов, поэтому атом либо переходит в состояние с меньшим уровнем энергии, либо остаётся на месте.

При помощи моделирования такого процесса ищется такая точка или множество точек, на котором достигается минимум некоторой числовой функции $F(\overline{x})$, где $ \overline{x} = (x_1, \dots, x_m) \in X$. 
Решение ищется последовательным вычислением точек $\overline{x_0}, \overline{x_0}, \dots ,$ пространства $X$; каждая точка, начиная с $\overline{x_1}$,
<<претендует>> на то, чтобы лучше предыдущих приближать решение. 
Алгоритм принимает точку $\overline{x_0}$ как исходные данные.
На каждом шаге алгоритм (который описан ниже) вычисляет новую точку и понижает значение величины (изначально положительной),
понимаемой как <<температура>>. Алгоритм останавливается по достижении точки, которая оказывается при температуре ноль.

Точка $\overline{x_{i+1}}$ по алгоритму получаеться на основе текущей точки $\overline{x_i}$
следующим образом. К точке $\overline{x_i}$ применяться оператор $A$, которым случайным образом модифицирует соответсвующую точку, 
в результате чего получатся новая точка $\overline{x^*}.$ Точка $\overline{x^*}$
становиться точкой $\overline{x_{i+1}}$ с вероятностью $P(\overline{x^*},\overline{x_{i+1}}),$ которая вычисляеться в соответсвии с распределение Гиббса:
\begin{equation*}
  P(\overline{x^*} \to \overline{x_{i+1}}|\overline{x_{i}}) = 
   \begin{cases}
     1, & F(\overline{x^*}) - \overline{x_i}<0, \\
     exp(-\frac{F(\overline{x^*}) - \overline{x_i}}{T_i}), & F(\overline{x^*}) - \overline{x_i} \geq 0.
   \end{cases}
\end{equation*}
Здесь $T_i>0$ элементы произвольной убывающей, сходящейся к нулю положительной последовательности,
которая задаёт аналог падающей температуры в кристалле.
Скорость убывания и закон убывания могут быть заданы по желанию создателя алгоритма.


Алгоритм имитации отжига похож на градиентный спуск,
но за счёт случайности выбора промежуточной точки должен попадать в локальные минимумы реже,
чем градиентный спуск.
Алгоритм имитации отжига не гарантирует нахождения минимума функции, однако при правильной политике генерации случайной точки в пространстве $X$, как правило, происходит улучшение начального приближения.
Результаты работы программы по поиску минимума значения функции Розенброка методом отжига приведены в следующей таблице $(x_0 = (0, 0), n = 100)$:
\\
\begin{tabular}{ | l | l | l | }
  \hline
  Кол-во запусков программы & Медиана & Минимальное значение \\ \hline
  100 & 0.93381 &  0.2682 \\
  500 & 0.65021 & 0.04298 \\
  1000 & 0.59129 & 0.0573 \\
  5000 & 0.62327 &  0.0153 \\
  \hline
\end{tabular}
\\
\subsection{Оценка точности вычислений}
Пусть $x=(x_1,x_2) --$ фактическая точка минимума, $\hat{x} = (\hat{x_1},\hat{x_2})$-- точка минимума, полученная методом случайного поиска.
Оценим $|x-\hat{x}|.$ Рассмотрим график исследуеом функции.

Исследуемая функция четная по $x_1,x_2,$ имеет несколько точек минимума, которые не являються граничными.
Тогда
$$ |x-\hat{x}|\leq\epsilon = \sqrt{\frac{p}{n}}.$$
Оценим $|f(x)-f(\hat{x})|$ через $|x-\hat{x}|.$
Поосколько $f$ -- непрерывна, то $f$ -- липшецева, следовательно:
$$|f(a)-f(b)| \leq || \nabla f||_{\infty}|a-b| = esssup_{a,b \in A} |\nabla f| |a-b| = max_{a,b \in A} |\nabla f| |a-b|, \forall a,b \in A.$$
Оценим $max_{x_1, x_2 \in A} |\nabla f|.$
$$|\frac{\partial f}{\partial x_1}| = |3x_1^2 sin(\frac{1}{x_1})-x_1 cos(\frac{1}{x_1})+10x_2^4 cos(\frac{1}{x_2})| \leq 3x_1^2 + |x_1| + 10x_2^4 \leq \sqrt{10}+10,$$
$$|\frac{\partial f}{\partial x_2}| = |10 x_1 x_2^2 cos(\frac{1}{x_2}) - 10 x_1 x_2^4 cos(\frac{1}{x_2})| \leq 10 |x_1|x_2^2+10|x_1|x_2^4\leq 10 \sqrt{17}.$$
Следовательно, $\nabla f =\sqrt{ (\frac{\partial f}{\partial x_1})^2 + (\frac{\partial f}{\partial x_2})^2} = \sqrt{(\sqrt{10}+10)^2+(10\sqrt{17})^2 } \leq 35.$ 
Окончательная оценка точности вычислений:
$$ |f(x)-f(\hat{x}| \leq 35 \sqrt{\frac{p}{n}}.$$
Занесём полученные результаты в таблицу:
\\
\begin{tabular}{ | l | l | l | }
  \hline
  $f_{min}$ & $N$ \\ \hline
  -0.8102 & $10^2$ \\  \hline
  -1.3333 & $10^3$ \\  \hline
  -1.3940 & $10^4$ \\  \hline
  -1.4067 & $10^5$ \\
  \hline
\end{tabular}
\\
\subsection{Примеры работы программы}
ы
%% ЗАДАНИЕ 8
\section{Задание 8}
\subsection{Формулировка задания}
Применить метод Монте-Карло к решению первой кравеой задачи для двумерного уравнения Лапласа
В единичном круге
\begin{equation*}
  \begin{cases}
    \Delta u=0,  & (x,y) \in D,\\
    u \bigg|_{\partial D} = f(x,y),\\
    u \in C^2(D),f \in C(\partial D),\\
    D = \{ x,y : x^2 + y^2 \leq 1\}.
  \end{cases}
\end{equation*}
Для функции $f(x,y) = x^2-y^2$ найти аналитическое решения и сравнить с
полученным по методу Монте-Карло.
\subsection{Алгоритм решения}
Для приближенного решения задачи выберем на плоскости достаточно мелкую квадратную сетку с шагом $h$.
В таком случае, координатами узлов сетки можно считать $x_j =jh, y_l =lh.$
\begin{definition}
  Будем называть узел сетки $(j,l)$ внутренним, если он и все четыре соседних с ним узла
  $(j-1,l), (j+1,l), (j, l-1), (j, l+1)$ принадлежит $D + \partial D, $ в противном случае узел $(j,l),$ 
  принадлежащий $D+\partial D,$ будем называть граничным.
\end{definition}

Во внутреннем узне $(x_i, y_i)$ уравнение Лапласа $u_{xx}+u_{yy}=0$
заменим разнотстным уравнением:
$$ \frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{h^2} + \frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{h^2}=0,$$
которое можно переписать в виде
$$ u_{i,j} = \frac{1}{4}(u_{i-1,j}+u_{i+1,j}+u_{i,j-1}+u_{i,j+1}).$$
В граничных узнал положим 
$$ u_{i,j} = f_{i,j}.$$
Представим себе частицу $M$, которая совершает равномерное случайное блуждание 
по узлам сетки. А именно, находясь во внутреннам узле $(x_i, y_j)$ сетки, 
эта частица за один переход с одинаковой вероятностью $1/4$ может переместиться в один из четырех
соседних узлов, причем каждый такой единичный переход случаен и не зависит от положения частицы и истории ее передвижений.
Будем считать, что блуждание заканчивается, как только частица попадет в граничный узел.
Пусть $P(i,j,p,q)$ -- вероятность того, что траектория частицы вышедшей из узла
$(x_i,y_j),$ закончиться в граничном узле $(x_p,y_q).$ Так как блуждание точки неизбежно закончиться на границе в первой же точке выхода ее на границу, тогда
$$ \sum_{(x_p,y_q) \in \partial D_k} P(i,j,p,q) =1,$$  
причем если $(p', q'), (p,q) \in \partial D_h,$то

\begin{equation*}
  P(p',q', p, q) = 
  \begin{cases}
    1, (p'-p)^2 + (q'-q)^2 =0,
    0, (p'-p)^2 +(q'-q)^2 \not = 0.
  \end{cases}
\end{equation*}
Составим сумму 
$$ v_{i,j} = \sum_{(x_p,y_q) \in \partial D_k} P(i,j,p,q) f_{pq}.$$
Если рассматривать функцию $f(x,y)$ как случайную величину, принимающую значения
$f_{pq}$ на границе $\partial D_h,$ то написанная выше сумма представляет собой математическое ожиние функции 
$f(x,y)$ на границе  $\partial D_h$ для траекторий, начинающихся в узле $(x_i,y_j).$
Тогда в силу закона больших чисел можно аппроксимировать математическое ожидание выборочным средним:
$$ v_{i,j} \approx \frac{1}{N} \sum_{k=1}^{N} f(x_p^{(k)},y_q^{(k)}).$$
Частица, начавшая свое случайное блуждание из внутреннего узла $(x_i,y_j),$ после первого шага с вероятностью 1/4, попажет в один 
четырех соседних узлов. Откуда по формуле полоной вероятности:
\begin{eqnarray}
  v_{i,j} = \frac{1}{4} \sum_{(x_p, y_q)\in \partial D_h} (  P(i-1,j,p,q) +P(i+1,j,p,q) +P(i,j-1,p,q)+P(i, j+1,p,q)    )f_{pq}=\\
  = \frac{1}{4}(v_{i-1,j}+v_{i+1,j}+v_{i,j+1}+v_{i,j-1}).
\end{eqnarray}
То есть во внутреннем узле $(x_i, y_j)$
$$ v_{i,j} = \frac{1}{4} (v_{i-1,j}+v_{i+1,j} + v_{i,j-1} + v_{i,j+1}),$$
в граничном узле 
$$ v_{i,j} = f_{i,j}.$$

По теореме о существовании решения внутренней задани Дирихле решение существует.

Найдем решение для конктретной функции $f(x,y) = x^2 - y^2.$ Будем искаоть его в виде
$u_{x,y} = Ax^2+By^2+C.$ Подсавив его в формулировку задани, получим 
следующие условия на коэффициаенты:
\begin{equation*}
  \begin{cases}
    A+B =  0, \\
    A-B = 2, \\
    B+C = -1;
  \end{cases}
\end{equation*}
Решая систему получаем, что $A = 1, B = -1, C=0.$ То есть функция $u(x,y) = x^2-y^2$ 
являеться решением задачи, причем это решение единственно.
Согласно приведённым выше выкладкам, численное решение может быть найдено по следующему алгоритму:
\begin{enumerate}
  \item Построим квадратную сетку на $[-1,1] \times [-1, 1]$ с шагом $\Delta.$
  \item Функцию во всех узлах, не принадлежащих кругу, положим равной $NaN.$
  \item Все токи круга разделим на граничные в внуренние:
  \begin{enumerate}
    \item В граничных точках положим $u(x,y) = f(x,y).$
    \item Значение в каждой внутренней точке получим следующим образом. Попав во
    внутреннюю точку $(x_i, y_j),$ проведем серию из $n$ случайных блужданий. Тогда
    $$u(x_i, y_j) = \frac{1}{n} \sum_{k=1}^{n} f(x_i^{(k)},y_i^{(k)}),$$
    где $(x_i^{(k)},y_i^{(k)})$ -- граничная точка, в который завершилось $k-$e блуждание.
  \end{enumerate}
\end{enumerate}
\subsection{Примеры работы программы}
\section{Задание 9}
\subsection{Формулировка задания}
Рассмотреть два вида процессов:
\begin{itemize}
  \item Винеровский процесс $W(t), t \in [0,1], W(0) =0.$
  \item Процесс Орешкйна-Уленбека $X(t), t \in [0,1], X(0) = X_0,$
  то есть стационарный марковский гауссовский процесс. Начальные значаения $X_0$ генеруются случайным образом так, 
  чтобы полученный процесс был стационарным.
\end{itemize}
Для данных гауссовских процессов 
\begin{enumerate}
  \item Найти ковариационную функцию и переходные вероятности.
  \item Моделировать независимые траектории процесса с данными переходными вероятностями методом добавления разбияния отрезка.
  \item Построить график трактории, не соеденияя точки ломанной, с целью получения визуального неприрывной линии.
\end{enumerate}
\subsection{Винеровский процесс}
\begin{definition}
  Пусть дано вероятностное пространоство $(\Omega, \mathbb{F}, \mathbb{P}).$
  Параметризованое семейство $\{ W_t\}_{t \in T}$ случаных величин 
  $$ W_t(\cdot) : \Omega \to \mathbb{R}, \ t \in T,$$
  где $T \subset [0, +\infty)$ интерпретируется как временной интервал, называется случайным процессом.
\end{definition}
\begin{definition}
  Пусть дан случайный процесс $\{ W_t\}_{t \in T}.$ Тогда он называется гаусовским, если для любых 
  $t_0, t_1, \dots, t_n \in T$ случайный вектор $(W_{t_1}, W_{t_2}, \dots, W_{t_n})$ имеет многомерное нормальное распределение.
\end{definition}
Определим винеровский процесс как гауссовский процесс в отрезке $[0,1]$ со средним 0 
и ковариционной функцией $cov(W(t_i),W(t_j)) = \min(t_i,t_j).$
Запишем основные свойства винеровского процесса:
\begin{itemize}
  \item $W_0 = 0$ почти наверное;
  \item $W_t$ являеться неприрывной функции от $t$;
  \item Приращения функции $W(t)$ независимы и меют нормальное равспределение со средним равным 0:$W_t - W_s \sim \mathcal{N}(0,1), s<t.$
\end{itemize}
Опредилим плотность $n-$ мерного нормального распределения с невырожденной ковариционнй матрицей.
\begin{definition}
  Пусть $x$ -- $n$-мерный вектор и $x \sim \mathcal{N}(m_x,R_).$ Тогда его плотность имеет вид
  $p(x) = \frac{1}{(2\pi)^{\frac{n}{2}} \sqrt{|R_x|}} e^{-\frac{1}{2}(x-m_x)^{T} R_x^{-1} (x_m_x)},$
  где $R_x$ -- коварионная матрица.
\end{definition}
Смоделируем винеровский процесс методом деления отрезка $[0, 1]$, в отношении $\alpha$, исходя из следующих соображений:
\begin{enumerate}
  \item В начальный момент времени $W_{t_0} = 0$ по определению;
  \item Генерируем $W_{t_1} = W_{t_1}- W_{t_0} \sim \mathcal{N}(0,1);$
  \item Рассмотрим отрезок $[t_1,t_2]$ его внутреннюю точку $t = t_1+\alpha(t_2-t_1)$ и условную плотность
  $$p_{W_t} (x | W_{t_1} = x_1, W_{t_2} = x_2) = \frac{p_{W_{t_1}, W_t, W_{t_2} } (x,1,x,x_2)}{ p_{W_{t_1}, W_{t_2}} (x_1, x_2) }.$$
  Обозначим векторы $\overline{x} = (x_1, x, x_2)^{T}$ и $\hat = (x_1, x_2)^T$
  и рассмотрим плотности вероятностей этиз векторов:
  $$ p_{W_{t_1}, W_t, W_{t_2}} = \frac{1}{(2\pi)^{\frac{3}{2}} \sqrt{|R_1|}} e^{-\frac{1}{2} \overline{x}^T R_1^{-1}\overline{x}},$$
  $$ p_{W_{t_1}, W_{t_2}} = \frac{1}{(2\pi)\sqrt{|R_2|}} e^{-\frac{1}{2} \hat{x}^T R_2^{-1}\hat{x}},$$
\end{enumerate}
где $R_1, R_2$ -- соответветсвующии матрицы ковариаций. Так как ковариационная функция имеет вид $k(s,t) = min(s,t),$ то находим выражения для 
$R_1$ и $R_2$:
\begin{equation*}
  R_1 =
  \begin{pmatrix}
  t_1 & t_1 & t_1\\
  t_1 & t & t\\
  t_1 & t & t_2
  \end{pmatrix}
\end{equation*}
\begin{equation*}
  R_2=
  \begin{pmatrix}
  t_1 & t_1\\
  t_1 & t_2
  \end{pmatrix}.
\end{equation*}
Вычислим определители и обратные матрицы для $R_1$ и $R_2$:
$$|R_1| = t_1 (t-t_1)(t_2-t),$$
\begin{equation*}
  R_1^{-1} =
  \begin{pmatrix}
  \frac{t}{t_1(t-t_1)} & -\frac{1}{t-t_1} & 0\\
  -\frac{1}{t-t_1} & \frac{t_2-t_1}{(t_2-t)(t-t_1)} & -\frac{1}{t_2-t}\\
  0 & \frac{1}{t_2-t} & \frac{1}{t_2-t}
  \end{pmatrix}
\end{equation*}
$$|R_2| = t_1(t_2-t_1),$$
\begin{equation*}
  R_2^{-1}=
  \begin{pmatrix}
  \frac{t_2}{t_1(t_2-t_1)} & -\frac{1}{t_2-t_1}\\
  -\frac{1}{t_2-t_1} & \frac{1}{t_2-t_1}
  \end{pmatrix}.
\end{equation*}
В итоге можнем подставить полученныенные выражения и получить выражение для $p_{W_t} (x | W_{t_1} = x_1, W_{t_2} = x_2)$.
\subsubsection{Алгоритм построения}
\begin{enumerate}
  \item $t_0, t_1=1, W_{t_0},$ разыгрываем $W_{t_1} \sim \mathcal{N}(0,1);$
  \item Рекурсивно делим отрезки $[t_0,t_1], [t_0,t], [t, t_1]$ и т.д. в отношений $\alpha$ к $1-\alpha$
  и разыгрываем случайные величины $W_t$ с условной плотнотью найденной в предидущем пункте(то есть имеющие нормально распределение с математическим ожиданием $(1-\alpha)x_1 + \alpha x_2$)
  и дисперсией $\alpha(1-\alpha)(t_2-t_1))$ до тех пор, пока не достигрем заданной точноси $t_{k+1}-t_k < \epsilon.$
\end{enumerate}
\subsection{Доверитиельные интервалы}
Чтобы убедиться в правильности работы программы, будем отрисовывать на графике 
доверительные интервалы для тракторий винеровского процесса. Доверительным интервало в данном случае будет 
$k_{1-\frac{\beta}{2}} \times [-\sqrt{t}, \sqrt{t}],$ где $k_{beta}--$ квантиль стандартного нормального распределения.
\subsection{Процес Орнштейна-Уленбека}
\begin{definition}
  Случайный процесс $\{ W_t \}_{t \in T}$ называеться стационарным, если конечномерные распределения инварианты отностиельно сдвига времени.
\end{definition}
\begin{definition}
  Гауссовкий процесс $\{W_t\}_{t \in T}$ называется процессом Орнштейна-Уленбека, если он являеться стационарным и марковским.
\end{definition}

Из стационарности процесса Орнштейна-Уленбека следует, что
$$ \mathbb{E}W_t = a, \ R(t,s) = R(|s-t|).$$
Без ограничения общности положим $a=0$.

Обозначим $\mathbb{D}W_t = \sigma^2,$ тогда $R(t,s)$ представима в виде $R(t,s) = \sigma^2 \rho(s,t),$
где $\rho(s,t)$ -- коэфициент корреляции.
\begin{theorem}
  Для того, чтобы последовательность $W_1, \dots, W_n$ нормально распределенных случайных величин была марковской,
  необходимо и достаточно, чтобы 
  $$ \rho_{j,k} = \rho_{j,i} \rho_{i,k}, \forall i,j,k : j \leq i < k \leq n,$$
  где $\rho_{i,j}$ -- коэфициент корреляции случайных величин $W_i$ и $W_j$.
\end{theorem}
Доказательство можно найти в [1].

В силу того, что процесс $W_t$ является марковским, получаем, что 
$$ \rho(s,t) = \rho(s,\tau) \rho(\tau,t).$$
Поскольку $R_(s,t) = R(|s-t|),$ то $\rho(s,t) = \rho(s-t).$ Тогда, введя замену 
$$ x = s - \tau,$$ 
$$ y = \tau -t,$$
преобразуем выражение для $\rho(s,t)$ к виду
$$ \rho(x+y) = \rho(x) \rho(y).$$ 
\begin{theorem}
  Пусть функция $u(t)$ определена при $t>0$ и ограничена на каждом конечном интервале. 
  Если $u(t)$ удовлетворяет соотношению $u(t+s) = u(t)u(s),$ то или $u(t) \equiv 0,$ или
  $u(t) = e^{-\lambda t},$ где $\lambda$ -- некоторая положительная константа.
\end{theorem}
Доказательство представлено в [4].

Если $\rho(t) \equiv 0,$ то $cov(W_t, W_s) =0,$ что равносильно тому, что $W_t$ независимы в совокупности(так как процесс являеться гауссрвским),
поэтому моделирование процесса Орнштейна-Уленбека заключается в моделировании случайных величин имеющих распрделение $\mathcal{N}(a,\sigma^2).$
Теперь рассмотри случай $\rho(s,t) = e^{\lambda |s-t|}, \lambda >0.$
Ковариационная функция процесса Орнштейна-Уленбека имеет вид
$$ R(s,t) = \sigma^2 e^{-\lambda |s-t|}.$$
Найдем переходную плотность 
$$ p_{W_t}(x_1|W_s= x_2) = \frac{p_{W_t,W_s}(x_1,x_2)}{p_{W_s}(x_2)}.$$
Поскольку $W_t$ -- гауссовский процесс, то 
$$p_{W_t, W_s}(x_1, x_2) = \frac{1}{2\pi |C|^{\frac{1}{2}}} \exp \{ -\frac{1}{2} (x, C^{-1}x) \},$$
$$ p_{W_s}(x) = \frac{1}{\sqrt{2\pi \sigma}} \exp{-\frac{x_2^2}{2\sigma^2}},$$
где $x = (x_1,x_2).$ Ковариционная матрица $C$ имеет вид

\begin{equation*}
  C=
  \begin{pmatrix}
  \sigma^2 & R(t,s)\\
  R(t,s) & \sigma^2
  \end{pmatrix}.
\end{equation*}
Тогда 
\begin{equation*}
  |C| = \sigma^4 - R^2(t,s), \ C^{-1} \frac{1}{|C|} 
  \begin{pmatrix}
  \sigma^2 & -R(t,s)\\
  -R(t,s) & \sigma^2
  \end{pmatrix}.
\end{equation*}
Поэтому 
$$ p_{W_t}(x_1|W_s = x_2) = \frac{1}{(2\pi (\sigma^2 - \frac{R^2(t,s)}{\sigma^2}) )^{\frac{1}{2}}} \exp \{ - \frac{(x_1-\frac{R(t,s)}{\sigma^2}x_2)^2}{2(\sigma^2 - \frac{R^2(t,s)}{\sigma^2})  },   \}$$
то есть 
$$ p(W_t|W_s=x_2) \sim \mathcal{N}{x_2 e^{-\lambda|t-s|}, \sigma^2 (1-e^{-2\lambda|t-s|}})).$$
Так как рассматриваемый процесс являеться марковским, то зная случайные величины $W_{t_1}, W_{t_2}$ мы можем сгенерировать 
случайную величину $W_t$, где $t_1 < t < t_2.$ Будем моделировать $W_t$ аналогично моделированию винеровского процесса.
Для упрощения положим $\alpha = \frac{1}{2}.$ Найдем условную плотность 
$$ p_{W_t} (x| W_{t_1} =x_1, W_{t_2} = x) = \frac{p_{W_{t_1},W_t, W_{t_2}}(x_1,x,x_2)}{p_{W_{t_1}, W_{t_2}}((x_1, x_2))},$$
где $t = \frac{t_1 + t_2}{2}$. Поскольку $W_t$ являеться гауссовским, то 
$$p_{W_{t_1}, W_t, W_{t_2}(x_1,x,x_2)} =  \frac{1}{(2\pi)^{\frac{3}{2}}|R_1|^{\frac{1}{2}}} \exp \{  -\frac{1}{2}(x_1,x,x_2)^T R_1^{-1}(x_1,x,x_2)  \},    $$
$$ p_{W_{t_1}, W_{t_2}}(x_1, x_2) = \frac{1}{2\pi |R_2|^{\frac{1}{2}}} \exp \{ -\frac{1}{2} (x_1, x_2)^T R_2^{-1}(x_1,x_2)\}$$
где 
\begin{equation*}
  R_1= \sigma^2
  \begin{pmatrix}
  1 & e^{-\lambda(t-t_1)} & e^{-\lambda(t_2-t_1)}\\
  e^{-\lambda(t-t_1)} & 1 & e^{-\lambda(t_2-t)}\\
  e^{-\lambda(t_2-t_1)} & e^{-\lambda(t_2-t)} 1 
  \end{pmatrix},
  R_2= \sigma^2
  \begin{pmatrix}
    1 & e^{-\lambda(t-t_1)} & e^{-\lambda(t_2-t_1)}\\
    e^{-\lambda(t_2-t_1)} & 1 
   \end{pmatrix},
\end{equation*}
После ряда преобразований получим 
$$ W_t \sim \mathcal{N} ((x_1+x_2) \frac{e^{-\frac{\lambda(t_2-t_1)}{2}}  }{1+e^{-\lambda(t_2-t_1)}}, \sigma^2 \frac{1-e^{\lambda(t_2-t_1)}}{1+e^{-\lambda(t_2-t_1)}} ).$$
В качестве $W_0$ и $W_1$ возьмем 
$$W_0 \sim \mathcal{N}(0,\sigma^2), \ W_1 \sim(x_0 e^{-\lambda T}, \sigma^2(1-e^{-2\lambda T}) ). $$
\subsection{Доверительные интервалы}
Чтобы убедиться в правильности работы программы, будем отрисовывать на графике доверительные интевалы для траекторий процесса 
Орнштейна-Уленбека. Доверительным интервалом в данном случае бдует 
$k_{1-\frac{\beta}{2}} \times \frac{\sigma^2}{\lambda} \times [-1, 1],$ где $k_{\beta}$ --
квантиль стандартного нормального распределения.
\subsection{Примеры работы программы}


\section{Задание 10}
\subsection{Формулировка задания}
Произвести фильтрацию одномерного процесса Орнштейна–Уленбека:
\begin{enumerate}
  \item  Используя генератор белого шума, добавить случайную ошибку с известной дисперсией к реализации процесса Орнштейна–Уленбека.
  \item  При помощи одномерного фильтра Калмана оценить траекторию процесса по за- шумленному сигналу. Параметры процесса и белого шума считать известными.
  \item   Рассмотреть случай, когда шум
  \begin{enumerate}
    \item Является гауссовским,
    \item Имеет распределение Коши.
  \end{enumerate}
\end{enumerate}
\subsection{Добавление случайной ошибки}
\begin{definition}
  Дискретным белым шумом называется последоваетльность 
  $\epsilon_1, \dots, \epsilon_n, \dots$ независимых одинаково распределнных случайных величин.
\end{definition}
Рассмотрим соотношение 
$$ x_{k+1} = f(x_k) + \omega(k),$$
где $\omega(k)$ -- случайная помеха, $x_k, \omega(k)$  независимы, 
$f(x_k) = \mathbb{E}(x_{k+1}|x_k).$ Пусть рассматриваеться марковский процесс, тогда 
совместная плотность по всем мементам времени 
$$ p(x_0, \dots, x_k) = p(x_k|x_{k-1}, \dots, x_0) \cdot  p(x_{k-1}|x_{k-2}, \dots, x_0)\cdot \dots \cdot p(x_1|x_0) \cdot p(x_0)=$$
$$=\{\text{марковский процесс} \} = p(x_{k}|x_{k-1}) \cdot p(x_{k-1}|x_{k-2}) \cdot p(x_{1}|x_{0}) \cdot p(x_0).$$

Обратим внимание, что в случае, когда шум имеет распределение Коши, фильтрацию провести не получится.
Это связанно с тем, что распределение Коши не имеет математического ожидания.
Далее будем рассматривать случай, когда шум является гауссовским ($\omega(k)$ и $x_k$ имеют гауссовское распределение).
\subsection{Фильтр Калмана}
Рассмотрим линейное стохастическое уравнение 
$$x_{k+1} = A_k x_k + \omega_k. $$

Поскольку случайные величины гауссовские, то для их полного описания достаточно знать их первые и вторые моменты.
Пусть имеется следующая система:
\begin{equation*}
  \begin{cases}
    x_{k+1} = A_kx_k + w_k, \\
    y_{k+1} = C_{k+1}x_{k+1} + v_{k+1},
  \end{cases}
\end{equation*}
причем $x_0, w_0, \dots, w_{N-1}, v_0, \dots, v_{n-1}$ независимы в совокупности.
$Y_{N-1} = (y_0, \dots, y_{N-1})^T -- $ все наблюдения, а $X_{N-1} = (x_0, \dots, x_{N-1}) --$
исходный процесс, который необходимо найти. Для этого воспользуемся фильтром Калмана.

Обозначим $\mathbb{E}x_0 = \overline{x_0}, \mathbb{D}x_0 =S, \mathbb{E}w_k = \mathbb{E}v_k = 0, \mathbb{D}w_k = M_k, \mathbb{D}v_k = N_k >0.$
Фильтр калмана имеет вид:
\begin{equation*}
  \begin{cases}
    \hat{x}_{k+1|k} = A_k \hat{x}_{k|k},\\
    \hat{x}_{k+1|k+1} = \hat{x}_{k+1|k}+R_{k+1|k}C_{k+1}^T(C_{k+1} R_{k+1|k}C_{k+1}^T +N_{k+1})^{-1} (y_{k+1}-C_{k+1} \hat{x}_{k+1|k})\\
    R_{k+1|k} = A_k R_{k|k} A_k^T + M_k\\
    R_{k+1|k+1} =  R_{k+1|k} -  R_{k+1|k}C_{k+1}^T(C_{k+1} R_{k+1|k}C_{k+1}^T +N_{k+1})^{-1}C_{k+1} R_{k+1|k},\\
    \hat{x}_{0|0} = \overline{x_0},
    R_{0|0} =S.
  \end{cases}
\end{equation*}

В нашей задаче $x_k$ -- процесс Орнштейна–Уленбека с параметрами 
$\sigma_W$ и $\lambda, y_{k+1}=x_{k+1} + v{k+1},$ где $v$ -- белый шум.
Пусть $\sigma_n^2 --$ его дисперсия. Тогда получаем, что $N_k = \sigma_n^2,$ а $C_k=1.$
Осталось найти $A_k$ и $M_k.$ Будем считать, что $t_{i+1}-t_i = \Delta t$ независимо от i.
Так как мы рассматриваем одномерный процесс Орнштейна–Уленбека, то $A_k, C_k$ являются скалярами,
то от их транспонированания ничего не меняеться. Обозначим $\mathbb{D}x_k=V_k.$
С одной стороный имеем:
$$  \mathbb{D}x_{k+1} = A_k^2\mathbb{D}x_k+\mathbb{D}w_k = A_k^2 V_k + M_k,$$
\begin{eqnarray}
  cov(x_{k+1},x_k) = \mathbb{E}(x_{k+1}x_k)- \mathbb{E}x_{k+1}\mathbb{E}x_k = \mathbb{E}(A_k x_k^2+w_{k+1}x_k)-A_k(\mathbb{E}x_k)^2 = \\
  \\ = \{ \mathbb{E} w_{k+1} = 0, w_{k+1} \text{и} x_k \text{независимы} \} = A_k(\mathbb{E}x_k^2-(\mathbb{E}x_k)^2) = A_k\mathbb{D}x_k = A_kV_k.
\end{eqnarray} 
С другой стороный, так как ковариационная функция процесса Орнштейна–Уленбека имеет вид 
$ R(t,x) = \sigma_w^2 e^{\lambda |t-s|},$ то получим следующую систему уравнений:
\begin{equation*}
  \begin{cases}
    A_k^2V_k + M_k = \sigma_W^2,\\
    A_kV_k = \sigma_W^2 e^{-\lambda \Delta t}, \\
    V_k = \sigma_W^2.
  \end{cases}
\end{equation*}
Получаем, что  $V_k = \sigma_W^2, A_k = e^{-\lambda\Delta t}, M_k = \sigma_W^2(1-e^{-2\lambda \Delta t}).$
Обратим внимание, что когда мы в предыдущем задании вводили процесс Орнштейна–Уленбека, то считали, что 
$\mathbb{D}x_k=  \sigma_W^2,$ что согласуеться с тем, что мы получили.

Тогда фильтр КАлмана для нашей задани имеет вид:
\begin{equation*}
  \begin{cases}
    \hat{x}_{k+1|k} = e^{-\lambda\Delta t} \hat{x}_{k|k},\\
    \hat{x}_{k+1|k+1} = \hat{x}_{k+1|k}+R_{k+1|k}(R_{k+1|k}^T +N_{k+1})^{-1} (y_{k+1}- \hat{x}_{k+1|k})\\
    R_{k+1|k} = e^{-2\lambda\Delta t}R_{k|k} + \sigma_W^2(1-e^{-2\lambda \Delta t})\\
    R_{k+1|k+1} =  R_{k+1|k} -  R_{k+1|k}( R_{k+1|k} +N_{k+1})^{-1} R_{k+1|k},\\
    \hat{x}_{0|0} = 0,\\
    R_{0|0} = \sigma_W^2.
  \end{cases}
\end{equation*}
Обозначив $h = R_{k+1|k}(R_{k+1|k}+\sigma_n^2)^{-1},$ получим итоговую систему:
\begin{equation*}
  \begin{cases}
    \hat{x}_{k+1|k} = e^{-\lambda\Delta t} \hat{x}_{k|k},\\
    \hat{x}_{k+1|k+1} = \hat{x}_{k+1|k}+R_{k+1|k}(R_{k+1|k}^T +N_{k+1})^{-1} (y_{k+1}- \hat{x}_{k+1|k})\\
    R_{k+1|k} = e^{-2\lambda\Delta t}R_{k|k} + \sigma_W^2(1-e^{-2\lambda \Delta t})\\
    R_{k+1|k+1} =  R_{k+1|k}(1-h),\\
    \hat{x}_{0|0} = 0,\\
    h = R_{k+1|k}(R_{k+1|k}+\sigma_n^2)^{-1} \\
    R_{0|0} = \sigma_W^2.
  \end{cases}
\end{equation*}
\subsubsection{Доверительные интервалы}

Чтобы убедиться в правильности работы программы, будем отрисовывать на графике
доверительные интервалы для траекторий процесса Орнштейна–Уленбека. Доверительным интервалом в данном случае будет
 $\hat{x} + k_{1-\frac{\beta}{2}} \times [-\sqrt{R_{k|k}}, \sqrt{R_{k|k}}],$ где $k_{\beta}$ —- квантиль
стандартного нормального распределения.
\subsection{Примеры работы программы}

\section{Задание 11}
\subsection{Формулировка задания}
Построить двумерное пуассоновское поле, отвечающее сложному пуассоновскому процессу:
\begin{enumerate}
	\item Первая интерпретация: система массового обслуживания. При этом, первая координата поля --- время поступления заявки в СМО (равномерное распределение), вторая --- время ее обслуживания (распределение $\chi^2$ c 10 степенями свободы).
	\item Вторая интерпретация: система массового обслуживания с циклической интенсивностью $\lambda(1 + \cos(t))$ и единичными скачками. Свести данную задачу моделирования неоднородного пуассоновского процесса при помощи метода Льюиса и Шеддера к моделированию двумерного пуассоновского поля, где первая координата имеет равномерное распределение, а вторая --- распределение Бернулли.
	\item Третья интерпретация: работа страховой компании. Первая координата --- момент наступления страхового случая (равномерное распределение), вторая координата --- величина ущерба (распределение Парето). Поступление капитала по времени линейно со скоростью $c > 0$, начальный капитал $W > 0$.
	\item Для каждой системы  рассмотреть всевозможные случаи поведения системы в зависимости от значения параметров.
\end{enumerate}
\subsection{Решение}
\subsubsection{Первая интерпретация: система массового обслуживания}
Пусть $\lambda$ --- интенсивность пуассоновского поля. Времена поступления заявок генерируются так, что $\Delta t_i = t_i - t_{i-1} \sim Exp(\lambda).$
\begin{definition}
	Распределением $\chi^2$ с $k$ степенями свободы называется распределение суммы квадратов $k$ независимых стандартных нормальных случайных величин.
\end{definition}
Время обслуживания каждой заявки $s_i$ независимы и генерируются как случайные величины с распределением $\chi^2(10)$.\\
Поскольку все заявки обрабатываются последовательно, время окончания обработки заявки, поступившей в момент времени $t_i$ можно найти следующим образом:
\begin{itemize}
	\item если к моменту поступления заявки предыдущая заявка уже обработана, то нужно к времени поступления текущей заявки прибавить время ее обработки:
	\[ Q_i = t_i + s_i. \]
	\item если предыдущая заявка еще не обработана, то нужно прибавить к времени конца обработки предыдущей заявки время обработки текущей:
	\[ Q_i = Q_{i-1} + s_i. \]
\end{itemize}
Обобщая вышесказанное, имеем:
\[ Q_i = t_i + \max (0,Q_{i-1} - t_i) + s_i. \]
Для каждой заявки будем считать количество людей в очереди.
\begin{itemize}
	\item если во время поступления $i-$й заявки очереди не было то положим $n_i = 0$.
	\item если предыдущая заявка еще не обработана, то:
	\[ n_i \neq Q_k: k < i \text{ и } Q_k > t_i, \]
	т.е. количество еще не выполненных к моменту времени $t_i$ заявок.
\end{itemize}
Поскольку время обработки еще одной заявки в среднем равно 10, а средний интервал между поступлениями заявок равен $\mathbb{E}\Delta_i = \frac{1}{\lambda}$, то при $\lambda < 0.1$ очереди практически не будет, а при $\lambda > 0.1$ очередь будет неограниченно расти.
\subsubsection{Вторая интерпретация: система массового обслуживания с циклической интенсивностью и единичными скачками}
Пусть $T_1,\dots,T_n\dots$ --- времена наступления некоторых событий, а $N(t_1,t_2)$ --- количество событий, произошедших в промежуток $[t_1,t_2]$. Заметим, что $T_{n+1} - T_n$ имеет функцию распределения $F(x) = 1 - \exp\{-\Lambda(t + x) - \Lambda(t)\}, x > 0$, где 
\[\Lambda(t) = \int_{0}^{t}\lambda(u)du = \lambda(t + \sin t)\]
неограниченно возрастает с ростом $t$. \\
$T_{n+1}$ распределено как $T_n + F^{-1}(U)$, где $U$ равномерно распределена на $[0,1]$. Заметим, что если записать $U$ как $1-\exp\{-E\}$, где $E$ --- экспоненциальная случайная величина с параметром $\lambda_E = 1$, то $T_{n+1}$ распределена как $\Lambda^{-1}(E + \Lambda(T_n)).$\\
Будем искать обратную функцию $\Lambda^{-1}(y)$ численно, так как аналитически это не представляется возможным $(\Lambda'(t) = \lambda_0(1 + \cos (t))$ почти всюду положительна, то есть функция возрастает). Такой метод моделирования неоднородного процесса Пуассона называется методом Льюиса-Шеддера.\\
Чтобы не искать обратную функцию, можно воспользоваться следующей модификацией метода Льюиса-Шеддера. Пусть имеется переменная $t$, в которой хранится текущее время (но не обязательно событие произошло строго в это время).
\begin{itemize}
	\item На каждом шаге генерируем случайную величину $\xi \sim Exp(2\lambda_0)$.
	\item Прибавляем к переменной $t$ величину $\xi$ и генерируем случайную величину $\eta = Bern(\frac{1 + \cos t}{2}).$
	\begin{itemize}
		\item [---] если она приняла значение 1, то полагаем $T_{i+1} = t$ и $i = i+1$
		\item [---] иначе ничего не делаем и повторяем процесс заново
	\end{itemize}
\end{itemize}
 
\subsubsection{Третья интерпретация: работа страховой компании}
\begin{definition}
	Случайная величина $X$ имеет распределение Парето с параметрами $x_m$ и $k$, если ее функция распределения имеет вид:
	\[ F_X(x) = 1 - \bigg(\frac{x_m}{x}\bigg)^k. \]
\end{definition}
Для моделирования случайной величины, имеющей распределение Парето, снова воспользуемся методом обратной функции.\\
Обратная функция для данной функции распределения имеет вид:
\[ F^{-1}_X(x) = \frac{x_m}{(1 - x)^\frac{1}{k}}. \]
Сгенерируем времена наступления страховых случаев на временном интервал $[0,T]$:
\[ 0 \leq t_1 \leq \dots \leq t_n < T, \]
причем $t_i - t_{i-1} \sim Exp(\lambda), \lambda > 0$ --- интенсивность потока страховых случаев.\\
Величину ущерба $s_i$ страхового случая в момент времени $t$ будем генерировать с помощью распределения Парето с параметрами $x_m$ и $k$. Случайную величину, распределенную по Парето, будем генерировать, воспользовавшись методом обратных функций:
\[ F^{-1}_\xi (y) = \frac{x_m}{(1-x)^\frac{1}{k}}. \]
Учтем, что если $Y \sim U[0,1],$ то и $(1 - Y) \sim U[0,1]$. Тогда случайная величина
\[ X = x_m Y^{-\frac{1}{k}}, \ Y \sim U[0,1] \]
имеет распределение Парето с параметрами $x_m$ и  $k$.\\
Величина капитала компании в момент времени $t$ выражается как
\[ W_t = W_0 + ct - s(t), \]
где $s(t)$ --- сумма величин ущерба страховых случаев, произошедших в моменты времени $t_i$ такие, что $t_i \leq t$. Время разорения --- случайная величина, задаваемая следующим условием:
\[ T = \min \{t > 0| W_t < 0 \}.\]
Выведем зависимость функции $W(t)$ от параметров $\lambda, x_m, k,W_0,c.$ Будем считать, что $k > 1$. Тогда 
\[\mathbb{E}W'(t) = c - \mathbb{E}'s(t) = c - \bigg( \mathbb{\bigg[\sum_{t_i < t}^{}s_i\bigg]} \bigg)' = c - \bigg(\frac{t}{\frac{1}{\lambda}}\mathbb{E}[s_i]\bigg)'=c - \frac{\lambda k x_m}{k -1}.\]
Таким образом:
\begin{itemize}
	\item $c(k-1) > \lambda kx_m$ капитал растет.
	\item $c(k-1) = \lambda kx_m$ система находится в положении равновесия.
	\item $c(k-1) < \lambda kx_m$ капитал уменьшается.
\end{itemize}

\subsection{Примеры работы программы}








\section{Библиография}

\begin{thebibliography}{1} 
  \bibitem{1} Колмогоров А.Н., Избранные труды, в 6 томах. Том 2. Теория вероятностей и математическая статистика. М., Математический институт им. В. А. Стеклова РАН.
  \bibitem{2} Кропачёва Н.Ю., Тихомиров А.С. Моделирование случайных величин: Метод. ука- зания, НовГУ им. Ярослава Мудрого, 2004.
  \bibitem{3} Феллер В. Введение в теорию вероятностей и её приложения, в 2−х томах. Т.1, М., Мир, 1984.
  \bibitem{4} Востриков И. В. Лекции по курсу «Теория идентификации», 2008.
\end{thebibliography}

\end{document}
